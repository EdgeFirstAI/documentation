{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EdgeFirst Studio","text":"<p>Welcome to the EdgeFirst Studio documentation.  EdgeFirst Studio provides a full end-to-end suite for visual perception at the edge, on low-powered embedded devices.  We provide hardware modules, models, and middleware for visual AI processing on embedded devices plus cloud tooling for automated dataset creation, model training, and deployment back to the embedded devices.</p> <p> </p>"},{"location":"datasets/","title":"EdgeFirst Datasets","text":"<p>Overview of EdgeFirst Studio Dataset workflow and pre-packaged datasets.</p>"},{"location":"datasets/format/","title":"EdgeFirst Dataset Format","text":"<p>This article describes the structure of the EdgeFirst Dataset Format. This structure provides two containers for storing sensor information and data annotations. The Dataset Storage Format is the container used to store sensor information from the MCAP recordings which includes the camera, LiDAR, Radar, and Depth estimations. The Dataset Annotation Format is the container used to store dataset annotations which  includes object label, 2D bounding boxes, segmentations masks, 3D bounding boxes, etc.</p>"},{"location":"datasets/format/#dataset-storage-format","title":"Dataset Storage Format","text":""},{"location":"datasets/format/#dataset-annotation-format","title":"Dataset Annotation Format","text":""},{"location":"datasets/tutorials/","title":"Dataset Tutorials","text":""},{"location":"datasets/tutorials/#capture-mcap-recordings","title":"Capture MCAP Recordings","text":""},{"location":"datasets/tutorials/#import-datasets-from-mcap","title":"Import Datasets from MCAP","text":""},{"location":"datasets/tutorials/#import-third-party-datasets","title":"Import Third-Party Datasets","text":""},{"location":"datasets/tutorials/#annotate-2d3d-boxes-and-2d-masks","title":"Annotate (2D/3D Boxes and 2D Masks)","text":"<p>This section will provide instructions for running auto-annotations on a set of images to generate 2D/3D bounding boxes and 2D masks for the annotations.  In addition, this section will also show the instructions for correcting any errors from the auto-annotations through manual audits.</p>"},{"location":"datasets/tutorials/#auto-annotations","title":"Auto Annotations","text":"<p>This section provides the instructions for auto-annotating a dataset.  When auto-annotating a dataset, a cloud server needs to be initiated to host the models that are used for auto-annotations.</p> <ol> <li>Select Cloud Instances from the tool options</li> </ol>"},{"location":"datasets/tutorials/#manual-annotations","title":"Manual Annotations","text":""},{"location":"datasets/tutorials/#export-datasets","title":"Export Datasets","text":""},{"location":"datasets/zoo/","title":"Dataset Zoo","text":"<p>EdgeFirst Datasets and commercially clean datasets which can be licensed from Au-Zone Technologies.</p> <p>Some example third-party datasets are provided for evaluation and research purposes only.  Refer to their respective licensing restrictions.</p>"},{"location":"enterprise/","title":"Deep View Enterprise","text":""},{"location":"fusion/","title":"Fusion Overview","text":"<p>The EdgeFirst Fusion provides a framework for training and deploying camera+radar fusion models.  The framework includes middleware for dataset capture, tooling for calibration, and the runtime for the low-level fusion model and integration with higher level sensor abstractions; such as painting radar PCD with classes identified by the low-level fusion model.</p> <p>The EdgeFirst Fusion model performs early fusion using the low-level raw radar range-doppler data cube along with the camera input.  The middleware stack also allows for mid-level fusion using radar PCD.  Which is best suited depends on the target application, generally speaking the low-level fusion model provides the most robust performance in scenarios where camera degradation is expected.  By contrast the mid-level fusion requires the camera to be operating, but does not require specialized training as the model is trained only on camera data with the radar data fused later in the pipeline.</p> <p></p>"},{"location":"fusion/#low-level-fusion","title":"Low-Level Fusion","text":"<p>The EdgeFirst Fusion model is an early low-level fusion model, it takes as input the low-level radar data cube and, optionally, the camera data.  The fusion of the inputs is performed within the main model before any object detection is performed on either input, this means the model predictions are always based on the combination of the inputs. This model is trained to predict object locations using a bird\u2019s eye view, that is top-down 2D world coordinates.  Using cartesian coordinates, targets are described by label (person, car, etc.) range in meters, and their x-location (left/right) relative to the sensor.</p> <p>The primary benefit of using low-level radar is the ability to make object detections with classification even without the camera.  This is critical for operation in adverse weather or lighting conditions where the camera cannot operate or is operating in a severely reduced capacity.  Another benefit of this model is in detecting objects in the radar signal which would normally be filtered out by traditional processing, for example detecting people very near strong radar reflectors such as heavy equipment or metallic structures.</p> <p>This video is an example of camera degradation and how the fusion model is robust to this type of occlusion.</p> <p>The low-level fusion model is trained to generate a 2D occupancy grid of recognized objects, which requires training against a ground truth of bird\u2019s eye view object annotations.  The EdgeFirst Studio tooling provides an AI-assisted workflow for creating these challenging annotations nearly autonomously, only requiring the user to guide the system by clicking examples of objects and the class to which they belong then letting the AI algorithms create the dataset for visual segmentation and bird\u2019s eye view annotations automatically.</p>"},{"location":"fusion/#data-cube","title":"Data Cube","text":"<p>The radar data cube is a 4D complex tensor containing the radar response after the initial range and doppler FFTs.  The EdgeFirst Fusion model can be trained for various cube configurations, details are covered in the EdgeFirst Fusion Model Training Guide.  Radar data cubes vary greatly between various modules and vendors, it is a key limitation of the proliferation of radar and especially low-level radar models in the public domain.  While an image captured on various models of an iPhone or Android could be used to train and test an imaging model without much worry about the details of the optical parameters, such is not the case with radar data cubes; the models are tightly coupled with the radar modules.  EdgeFirst Fusion has well parameterized inputs to allow training with various sensor configurations, once trained the model is locked in to the particular radar module and configuration.  This also affects the dataset, care should be taken to keep datasets using compatible radar configurations, not mixing long and ultra-short range captures within a dataset for example.</p> <p>While the radar data cube adds complexity to the dataset capture and model training, the benefits are significantly richer input features for the model.  While radar point clouds have very few scalar features (speed, power, noise, RCS) the radar cube offers a large 4D complex tensor as input to the AI models.  This allows the model to learn to detect and discriminate objects which would not be possible with only radar point clouds.</p> <p>The data cube is not easily interpreted, it is visually not intuitive without much experience.  Special handling is required by AI models as the input representation is unlike images, there is no direct correlation between the input and output representations. A good example of the difficulty of handling the radar cube is when locating objects according to azimuth and elevation, these must be calculated through the timing differences of the same signal returns across multiple receive antennas.  This is handled automatically by EdgeFirst Fusion but classical algorithms would need to handle this directly along with knowledge of the antenna array's physical dimensions.</p> <p>The radar data cube contains a pair of sequences (A/B) each of which contains the receiving antenna\u2019s range-doppler matrix as complex values.  The sequences represent different TX patterns, most notably the A sequence runs a single transmitter while the B sequence runs all transmitters.  So generally a model will be trained on the B sequence, or possibly the A/B sequence, while the A sequence would be reserved for special cases where we do not require resolving multiple objects at the same range.</p> <p>The cube can be thought of as 8 range-doppler matrices.  The shape of the cube is therefore [seq, rx, range, doppler, complex] where complex is the pair of real and imaginary values, these pairs can either be interpreted as separate values or as first-class complex numbers depending on the programming language and framework used to perform calculations.   In the case of EdgeFirst Fusion the AI model training and inferencing is done by treating complex values as pairs of values instead of complex therefore we do not require special complex number handling of the AI frameworks or model optimizers.</p>"},{"location":"fusion/#occupancy-grid-encoding","title":"Occupancy Grid Encoding","text":"<p>The EdgeFirst Fusion model is trained to produce an occupancy grid.  The occupancy grid is effectively a segmentation mask, we use segmentation loss functions and present the ground truth as a segmentation task.  The resolution of the segmentation mask represents the occupancy grid, each pixel is a cell in the occupancy grid.  For binary classification, such as pedestrian detection, each cell or pixel is a single scalar giving a score from 0..1 where 0 is background and 1 is target object.  For multi-label classification each cell is a vector of the possible labels with their respective scores, these can be used to select a top label for the cell or to report all the scores; this is use case dependent.</p> <p>Using the occupancy grid a user application can determine the location, including the range, of detected objects.  The occupancy grid only provides location and label, but we can use the occupancy grid along with the radar targets, or clusters, to match targets within the occupancy cell and determine some additional parameters such as speed, power, and RCS.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Welcome to EdgeFirst Studio, this tutorial will walk you through the full end-to-end workflow. Though not required, we urge you to follow along with an appropriate edge device to test the models on the actual hardware.  If you don't have such a device, that's okay, we'll show you how to run the models on a PC using the same applications and APIs you would be using on the embedded device so you may get familiar with this environment.</p> <p>If you have recently received one of these EdgeFirst Modules you may want to first start with the EdgeFirst Modules Quickstart then come back once you're ready to get started with the cloud tools.</p>"},{"location":"getting_started/#quickstart","title":"Quickstart","text":""},{"location":"getting_started/#1-sign-up","title":"1. Sign up","text":"<p>Start by creating your EdgeFirst Studio Account.</p>"},{"location":"getting_started/#2-create-a-project","title":"2. Create a project","text":"<p>Once you're logged in, create your first project. Provide a name and description of the project  that reflects your goals. </p> Create a New Project"},{"location":"getting_started/#3-try-our-dataset","title":"3. Try our dataset","text":""},{"location":"getting_started/#4-train-a-model","title":"4. Train a model","text":""},{"location":"getting_started/#5-validate-the-model","title":"5. Validate the model","text":""},{"location":"getting_started/#6-deploy-the-model","title":"6. Deploy the model","text":""},{"location":"getting_started/#create-your-own-experiment","title":"Create Your Own Experiment","text":""},{"location":"getting_started/#1-record-your-dataset","title":"1. Record your dataset","text":""},{"location":"getting_started/#2-label-your-dataset","title":"2. Label your dataset","text":""},{"location":"getting_started/#3-combine-your-dataset","title":"3. Combine your dataset","text":""},{"location":"getting_started/#4-train-your-model","title":"4. Train your model","text":""},{"location":"getting_started/#5-validate-your-model","title":"5. Validate your model","text":""},{"location":"getting_started/#6-deploy-your-model","title":"6. Deploy your model","text":""},{"location":"getting_started/dataset/","title":"Dataset Tutorials","text":"<p>Describing the various dataset workflows in EdgeFirst Studio from capture to annotation and dataset management (curation).</p>"},{"location":"getting_started/developer/","title":"Middleware Developer Tutorials","text":""},{"location":"getting_started/fusion/","title":"Sensor Fusion Tutorials","text":""},{"location":"getting_started/modelpack/","title":"ModelPack Tutorials","text":""},{"location":"getting_started/tutorials/","title":"Tutorials","text":"<p>These EdgeFirst Studio tutorials cover various aspects of the workflow in deeper detail than the general overview.</p>"},{"location":"middleware/","title":"Middleware Overview","text":"<p>The EdgeFirst Perception Middleware is a modular software stack designed as a collection of services communicating over a ROS2-like communication middleware called Zenoh. The various application services  are each focused on a general task.  For example a camera service is charged with interfacing with the  camera and ISP (Image Signal Processor) to efficiently deliver camera frames to other services which require access to the camera.  The camera service is also responsible for encoding camera frames using  a video codec into H.265 video for efficient recording or remote streaming, this feature of the camera service can be configured or disabled if recording or streaming are not required.</p> <p>The middleware services communicate with each other using the Zenoh networking middleware which provides  a highly efficient publisher/subscriber communications stack.  While we do not directly depend on ROS2 the services do encode their messages using the ROS2 CDR (Common Data Representation). The middleware uses the ROS2 standard schemas where applicable and augment them with custom schemas where required. The Recorder and Foxglove chapters go into more detail on how this allows efficient streaming and  recording of messages and interoperability with industry standard tools.</p> <pre><code>graph LR\n    camera --&gt; model[\"vision model\"] --&gt; zenoh    \n    radarpub --&gt; fusion[\"fusion model\"] --&gt; zenoh\n    lidarpub --&gt; zenoh\n    camera --&gt; fusion\n    radarpub --&gt; zenoh\n    camera --&gt; zenoh    \n    model --&gt; fusion\n    navsat --&gt; zenoh\n    imu --&gt; zenoh\n    zenoh --&gt; recorder --&gt; mcap\n    zenoh --&gt; webui --&gt; https\n    zenoh --&gt; user[\"user apps\"]\n    https --&gt; user</code></pre>"},{"location":"middleware/api/","title":"API Reference","text":"<p>This is the reference manual for the EdgeFirst Middleware Schemas and dependencies.  The documentation is generated from the Python API of the schemas, which includes the dependencies from ROS2 and Foxglove. The Rust API is documented on docs.rs/edgefirst-schemas.</p> <p>We document all the schemas used by the EdgeFirst Middleware, including the upstream ones.  The source code for all schemas currently in use are available from these three repositories.</p> <ul> <li>EdgeFirst Schemas</li> <li>Foxglove Schemas</li> <li>ROS2 Common Interfaces</li> </ul> <p>The topics documentation gives a breakdown of the available topics and the schemas they use, referencing them in this API Reference.</p> <ul> <li>EdgeFirst Messages</li> <li>Foxglove Messages</li> <li>ROS2 Common Interfaces<ul> <li>Builtin Interfaces</li> <li>Geometry Messages</li> <li>Navigation Messages</li> <li>Sensor Messages</li> <li>Standard Messages </li> </ul> </li> </ul>"},{"location":"middleware/custom/","title":"Custom Topics &amp; Schemas","text":"<p>This chapter documents how custom topics and schemas are defined and implemented.  The same principles used to implement the EdgeFirst Perception Middleware services can be used to implement custom user applications which can run directly on the embedded target or on another device networked with it.</p>"},{"location":"middleware/python_examples/","title":"Python Examples","text":"<p>We provide two GitHub repositoiries to show how to interact with MCAP files and ROS2 topics over Zenoh using Python.  The first repository, the <code>MCAP-py</code> repository, has a <code>viewer.py</code> script to show how to use Python to interact with MCAP files generated by the Raivin.  The second repository, the <code>topicsub-py</code> repository, has a number of scripts demonstrating how to connect and report on the various topics generated by a Raivin.</p>"},{"location":"middleware/python_examples/#mcap","title":"MCAP","text":"<p>The <code>MCAP-py</code> repository has a single script, <code>viewer.py</code> that demonstrates the following functionality: * loads MCAP file * parses the MCAP file * searches the MCAP file for necessary topics   * <code>/camera/info</code> which is used to get the length and width of the images,   * <code>/camera/h264</code> which is used to get the images to display   * <code>/model/boxes2d</code> which is used to get the object detection bounding boxes * decodes the bounding boxes from the <code>/model/boxes2d</code> topic and draws them on the image found from the <code>/camera/h264</code> topic</p> <p>It uses the MCAP Python Library and our EdgeFirst Schemas Library.</p>"},{"location":"middleware/python_examples/#environments","title":"Environments","text":"<p>This has been tested on Python version 3.10.12 on Windows 10 and Ubuntu 22.04 on Windows Subsystem for Linux (WSL).</p>"},{"location":"middleware/python_examples/#setup","title":"Setup","text":"<p>Please follow the README.md file for specific installation instructions.</p> <p>After you download the repository, it is recommened you create a Python virtual environment in the directory, then install the requirements. <pre><code>$ python -m venv venv\n$ source venv/bin/activate\n(venv) $ pip install --upgrade -r requirements.txt\n</code></pre></p>"},{"location":"middleware/python_examples/#usage","title":"Usage","text":"<p>The usage of the <code>viewer.py</code> script is as follows: <pre><code>(venv) kris@AUZONEDEV02:~/mcap-py$ python viewer.py -h\nusage: viewer.py [-h] [-m [MODEL]] [-s SCALE] [-t THICKNESS] [-b] mcap_file\n\nProcess MCAP to view images with bounding boxes.\n\npositional arguments:\n  mcap_file             MCAP that needs to be parsed\n\noptions:\n  -h, --help            show this help message and exit\n  -m [MODEL], --model [MODEL]\n                        Run the MCAP frames through a custom ONNX model to display bounding box.\n  -s SCALE, --scale SCALE\n                        Resizing factor to view the final image 0.1-1.0. Default: 1.0\n  -t THICKNESS, --thickness THICKNESS\n                        Choose the thickness of the bounding box. Default: 2\n  -b, --display_bbox    Choose to view the bounding box. Default: True\n</code></pre></p>"},{"location":"middleware/python_examples/#codeblocks-of-note","title":"Codeblocks of Note","text":"<p>To read an MCAP file, all you need to do is open the MCAP file and pass it as an argument to <code>mcap.reader.make_reader()</code>. <pre><code>with open(mcap_file, \"rb\") as f:  # Open the MCAP file for reading\n    logger.info(f\"Opening {mcap_file}\")\n    reader = make_reader(f)  # Create a reader object for reading messages\n</code></pre> You can then use the MCAP reader object in your code, for example, to read its summary: <pre><code>sum = reader.get_summary()\n</code></pre> or to iterate through its messages: <pre><code>for schema, channel, message in reader.iter_messages():  # Iterate over messages in the file\n    if channel.topic == \"/camera/info\" and scale_not_set: \n</code></pre> The summary contains a list of topics that can be checked: <pre><code>def is_topic_present(summary:Summary, topic:str) -&gt; bool:\n    for id, channel in summary.channels.items():\n        if channel.topic == topic:\n            logger.info(f\"Found topic {topic}\")\n            return True\n    logger.error(f\"Did not find topic {topic}\")\n    return False   \n</code></pre> The message data from <code>reader.iter_messages()</code> can be converted into scriptable outputs using our Edgefirst Schemas.  Importing them as:</p> <p><pre><code>from edgefirst.schemas.sensor_msgs import CameraInfo as Info  # Custom message module for camera information  \nfrom edgefirst.schemas.foxglove_msgs import CompressedVideo  # Custom message module for H264 decoding \nfrom edgefirst.schemas.edgefirst_msgs import Detect # Custom message module for detection\n</code></pre> We can then use: <pre><code>image_data =  CompressedVideo.deserialize(message.data) # Deserialize the message data to get H264 frames\nmcap_image = get_image(bytes(image_data.data), frame_position)  # Get the image frame from the message\n</code></pre> The byte-string of the <code>image_data.data</code> can then be used by the PyAV library to generate images and Matplotlib to display them.</p> <p>We can get image size (height and width) a similar way: <pre><code>def set_image_size(message, scale):\n    global frame_height  # Access the global variable\n    frame_height = int(Info.deserialize(message.data).height*scale)  # Update the frame height\n    global frame_width  # Access the global variable\n    frame_width = int(Info.deserialize(message.data).width*scale)  # Update the frame width\n    return False  # Return False to indicate that scale is set\n</code></pre> We can get the box decodes as well:  <pre><code>boxes = Detect.deserialize(message.data)\nbox_time = boxes.header.stamp.sec + (boxes.header.stamp.nanosec / 1e9) # Get the box time\nboxes_map[box_time] = boxes.boxes\nclosest_time = get_closest_time(boxes_map, frame_time)\nif mcap_image is not None:\n    logger.info(f\"Found {len(boxes_map[closest_time])} boxes at time {closest_time}\")\nfor points in boxes_map[closest_time]:  # Iterate over annotation points\n    if points and mcap_image is not None:  # Check if points and image are available\n        x = int((points.center_x - points.width / 2) * frame_width/scale)\n        y = int((points.center_y - points.height / 2) * frame_height/scale)\n        w = int(points.width * frame_width/scale)\n        h = int(points.height * frame_height/scale)\n</code></pre></p>"},{"location":"middleware/python_examples/#topicsub-py","title":"Topicsub-py","text":"<p>This repository contains a number of Python scripts that read the network topics generated by the ROS2 messaging over Zenoh DDS and demonstrates the following functionality: * lists the topics a Raivin is sending data out on * lists the schemas the topics use to encode data on * outputs from the Raivin:   * the PointCloud information from the Radars Target topic <code>rt/radar/targets</code>   * GPS information from <code>rt/gps</code>   * IMU information from <code>rt/imu</code>   * processed radar data in summary and raw output from <code>rt/fusion/targets</code>   * camera information from <code>rt/camera/info</code>   * detect boxes from <code>rt/model/boxes2d</code></p> <p>It uses the Zenoh Python Library and our EdgeFirst Schemas Library.</p>"},{"location":"middleware/python_examples/#environments_1","title":"Environments","text":"<p>This has been tested on the Raivin device itself (release 2025-Q1, Python version 3.10.15), Python version 3.10.12 on Windows 10, and Ubuntu 22.04 on Windows Subsystem for Linux (WSL).</p>"},{"location":"middleware/python_examples/#setup_1","title":"Setup","text":"<p>Please follow the README.md file for specific installation instructions.  It is recommended you create a Python virtual environment to run the scripts in.</p>"},{"location":"middleware/python_examples/#libraries-not-present-on-raivin","title":"Libraries not present on Raivin","text":"<p>If the Raivin does not have the Python libraries installed and does not have external Internet access, the packages can be downloaded on a Internet-connected PC with the following command: <pre><code>pip download --no-cache-dir -r requirements.txt --platform=manylinux_2_28_aarch64 --platform=manylinux_2_17_aarch64 --only-binary=:all:\n</code></pre> Which should output 4 files: <pre><code>edgefirst_schemas-1.2.4-py3-none-any.whl\neclipse_zenoh-1.2.1-cp38-abi3-manylinux_2_28_aarch64.whl\nnumpy-2.2.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\npycdr2-1.0.0-py3-none-any.whl\n</code></pre> These wheel files (as well as the Python topics scripts) can copied onto the device via SSH secure copy.  Once there, they can be installed in a virtual enviornment: <pre><code>$ python -m venv venv\n$ source venv/bin/activate\n(venv) $ pip install pycdr2-1.0.0-py3-none-any.whl\n(venv) $ pip install edgefirst_schemas-1.2.4-py3-none-any.whl\n(venv) $ pip install eclipse_zenoh-1.2.1-cp38-abi3-manylinux_2_28_aarch64.whl\n(venv) $ pip install numpy-2.2.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n</code></pre></p>"},{"location":"middleware/python_examples/#start-the-zenohd-router","title":"Start the Zenohd Router","text":"<p>The Zenohd Router will need to be started to send messages off-device.  On the Raivin, run the following command: <pre><code>sudo systemctl start zenohd\n</code></pre> Or start the service from the Raivin's WebUi: 1. Click the Settings \"gear\" icon in the top right of the Raivin UI: 2. Click the \"Service Status\" card on the Settings Page. 3. Toggle the \"Start/Stop\" switch on the \"zenohd\" card to \"Start\"</p> <pre><code>You do not need to enable the Zenohd Router service unless you want it to start on boot.  If you do, either run the command `sudo systemctl enable zenohd` or, on step 3 above, toggle the \"Enable/Disable\" switch on the \"zenohd\" card to \"Enable\".\n</code></pre> <p>Confirm the router is running with the command: <pre><code>torizon@verdin-imx8mp-07129940:~$ systemctl status zenohd\n\u25cf zenohd.service - Zenoh Message Router\n     Loaded: loaded (/usr/lib/systemd/system/zenohd.service; disabled; vendor preset: disabled)\n     Active: active (running) since Mon 2025-03-17 21:49:18 UTC; 1 day 21h ago\n   Main PID: 3480 (zenohd)\n      Tasks: 11 (limit: 3772)\n     Memory: 4.6M\n     CGroup: /system.slice/zenohd.service\n             \u2514\u2500 3480 /usr/bin/zenohd --config /etc/zenohd.yaml\n</code></pre> Or on the \"Services Status\" page by seeing the status set to \"RUNNING\" on the 'zenohd' card.</p>"},{"location":"middleware/python_examples/#usage_1","title":"Usage","text":"<p>The usage of most of the scripts is as follows: <pre><code>python .\\fusionraw.py -h\nusage: fusionraw.py [-h] [-c CONNECT]\n\nImu Example\n\noptions:\n  -h, --help            show this help message and exit\n  -c CONNECT, --connect CONNECT\n                        Connection point for the zenoh session, default='tcp/127.0.0.1:7447'\n</code></pre> Running the scripts on a PC to connect to a Raivin with the Zenoh router running would have the following command:</p> <p><pre><code>python .\\fusionraw.py -c tcp/&lt;IP address or hostname&gt;:7447\n</code></pre> <pre><code>The IP address can be:\n* IPv4 address (e.g. `python .\\topics.py tcp/10.10.41.195:7447` )\n* IPv6 address (e.g. `python .\\topics.py tcp/[fe80::c91e:a40:1c9c:5a7c]:7447` )\n* the hostname (e.g. `python .\\topics.py -c tcp/verdin-imx8mp-07130049.local:7447`)\n</code></pre></p> <p>Most scripts need the user to hit CTRL-C to stop.</p>"},{"location":"middleware/python_examples/#codeblocks-of-note_1","title":"Codeblocks of Note","text":"<p>The scripts start trying to connect to the Raivin: <pre><code>try:\n    cfg = zenoh.Config()\n    cfg.insert_json5(\"mode\", \"'client'\")\n    cfg.insert_json5(\"connect\", '{ \"endpoints\": [\"%s\"] }' % args.connect)\n    session = zenoh.open(cfg)\nexcept zenoh.ZError as e:\n    print(f\"Failed to open Zenoh session: {e}\")\n    sys.exit(1)\n</code></pre> and close the session during exit: <pre><code>    try:\n        while True:\n            time.sleep(0.1)\n    except KeyboardInterrupt:\n        print(\"\\nExiting...\")\n        session.close()\n        sys.exit(0)\n</code></pre> Most scripts subscribe to a single topic: <pre><code>sub = session.declare_subscriber('rt/fusion/model_output', fusion_listener)\n</code></pre> And use the <code>edgefirst.schemas</code> to decode the output messages into manipulatible objects: <pre><code>from edgefirst.schemas.edgefirst_msgs import Mask\n\ndef fusion_listener(msg):\n    fusion = Mask.deserialize(bytes(msg.payload))\n    print(f\"\\nMask Data:\")\n    print(f\"Dimensions: {fusion.width} x {fusion.height}\")\n    print(f\"Encoding: {fusion.encoding}\")\n    print(f\"Length: {fusion.length}\")\n</code></pre></p>"},{"location":"middleware/topics/","title":"Middleware Topics","text":"<p>The EdgeFirst Perception Middleware is built on a principle of having modular services handling their specialized tasks. For example the camera service is tasked with configuring the camera source and publishing camera frames, the model service subscribes to the camera service to receive camera frames which are then processed by the model and, in turn, publish the model results.  There are numerous such services, some can be run in multiple instances to support multiple camera inputs or multiple parallel models.</p> <pre><code>graph LR\n    camera --&gt; model[\"vision model\"] --&gt; zenoh    \n    radarpub --&gt; fusion[\"fusion model\"] --&gt; zenoh\n    lidarpub --&gt; zenoh\n    camera --&gt; fusion\n    radarpub --&gt; zenoh\n    camera --&gt; zenoh    \n    model --&gt; fusion\n    navsat --&gt; zenoh\n    imu --&gt; zenoh\n    zenoh --&gt; recorder --&gt; mcap\n    zenoh --&gt; webui --&gt; https\n    zenoh --&gt; user[\"user apps\"]\n    https --&gt; user</code></pre> <p>These middleware applications publish messages and subscribe to messages from other publishers on what is referred to as a topic. Services will often publish to multiple topics within a namespace, for example the camera service uses the <code>/camera</code> namespace to publish a few topics such as <code>/camera/info</code> which publishes information about the camera such as resolution and intrinsic calibration parameters.  The camera service also publishes <code>/camera/dma</code> which handles zero-copy of camera frames between consumers, it can also publish the <code>/camera/h264</code> or <code>/camera/jpeg</code> topics for cases where compressed frames are required, such as when recording or streaming the camera over the network.</p> <p>Topics are identified by these path-like names and the underlying discovery and network connections to publish and subscribe over topics is handled by the Zenoh library.  Zenoh topics are by default available only on the local device, a service named <code>zenohd</code> can be run to allow remote connections to the device's topics through a router interface defined as part of the Zenohd protocol.  Messages published over Zenoh support various encodings defined through MIME types.  The EdgeFirst Middleware uses the Common Data Representation (CDR) encoding for messages, this is an open standard encoding and the same used by ROS2.  The CDR encoding uses schemas to represent each type of message, the EdgeFirst Middleware uses the ROS2 common interfaces whenever possible and provides custom schemas when required.  The schemas are published on Github and we provide pre-made bindings for Python and Rust.  The schemas can be installed into a ROS2 system, we cover EdgeFirst Middleware with ROS2 integration in the Zenoh ROS2 Bridge chapter.</p> <p>User applications interface with the EdgeFirst Middleware by subscribing to the appropriate topics.  For example, if we need an application to display the camera feed with bounding boxes drawn from the detection model we would write an application which subscribes to the camera and model topics.  This application would be responsible for drawing the camera pixels and then drawing the bounding box pixels over the camera and finally displaying the results for the user.  We provide a few examples of such applications, the first you're likely to see is the Web User Interface (REFER TO WEBUI CHAPTER).  Our sample code includes many examples which use the Rerun framework for drawing and demonstrate how to subscribe to topics and how to interpret the results, such as reading bounding boxes and drawing them over the camera feed.  You'll see these examples using Rerun for display througout our examples, but there is no direct connection to rerun and user applications could use any UI of their choosing.</p>"},{"location":"middleware/topics/#camera-topic","title":"Camera Topic","text":"<p>The camera topic is published under the <code>/camera</code> namespace and offers the following sub-topics.  Some topics are optional and might not be available on the current system, refer to the camera service configuration documentation for details.</p>"},{"location":"middleware/topics/#camerainfo","title":"/camera/info","text":""},{"location":"middleware/topics/#cameradma","title":"/camera/dma","text":""},{"location":"middleware/topics/#camerah264","title":"/camera/h264","text":""},{"location":"middleware/topics/#camerajpeg","title":"/camera/jpeg","text":""},{"location":"middleware/topics/#radar-topic","title":"Radar Topic","text":""},{"location":"middleware/topics/#lidar-topic","title":"LiDAR Topic","text":""},{"location":"middleware/api/builtin_interfaces/","title":"Builtin Interfaces","text":"<p>These are primitive types which do not have schema definitions but are the basic primitive building blocks for schemas.</p>"},{"location":"middleware/api/builtin_interfaces/#duration","title":"Duration","text":"<p>Duration defines a period between two time points. Messages of this datatype are of ROS Time following this design: https://design.ros2.org/articles/clock_and_time.html</p>"},{"location":"middleware/api/builtin_interfaces/#edgefirst.schemas.builtin_interfaces.Duration.nanosec","title":"nanosec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nanosec: uint32 = 0\n</code></pre> <p>The nanoseconds component, valid in the range [0, 10e9).</p>"},{"location":"middleware/api/builtin_interfaces/#edgefirst.schemas.builtin_interfaces.Duration.sec","title":"sec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sec: int32 = 0\n</code></pre> <p>The seconds component, valid over all int32 values.</p>"},{"location":"middleware/api/builtin_interfaces/#time","title":"Time","text":""},{"location":"middleware/api/builtin_interfaces/#edgefirst.schemas.builtin_interfaces.Time.nanosec","title":"nanosec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nanosec: uint32 = 0\n</code></pre> <p>The nanoseconds component, valid in the range [0, 10e9).</p>"},{"location":"middleware/api/builtin_interfaces/#edgefirst.schemas.builtin_interfaces.Time.sec","title":"sec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sec: int32 = 0\n</code></pre> <p>The seconds component, valid over all int32 values.</p>"},{"location":"middleware/api/edgefirst_msgs/","title":"EdgeFirst Messages","text":""},{"location":"middleware/api/edgefirst_msgs/#box","title":"Box","text":""},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.center_x","title":"center_x  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center_x: float32 = 0\n</code></pre> <p>Normalized x-coordinate of the center</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.center_y","title":"center_y  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center_y: float32 = 0\n</code></pre> <p>Normalized y-coordinate of the center</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.distance","title":"distance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance: float32 = 0\n</code></pre> <p>Distance of object (if known)</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.height","title":"height  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>height: float32 = 0\n</code></pre> <p>Normalized height of the box</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.label","title":"label  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label: str = ''\n</code></pre> <p>object label</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.score","title":"score  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score: float32 = 0\n</code></pre> <p>confidence score for detection</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.speed","title":"speed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>speed: float32 = 0\n</code></pre> <p>Speed of object (if known)</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.track","title":"track  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>track: Track = default_field(Track)\n</code></pre> <p>object tracking, each track includes ID and lifetime information</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Box.width","title":"width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>width: float32 = 0\n</code></pre> <p>Normalized width of the box</p>"},{"location":"middleware/api/edgefirst_msgs/#detect","title":"Detect","text":""},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Detect.boxes","title":"boxes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>boxes: sequence[Box] = default_field([])\n</code></pre> <p>Array of detected object bounding boxes</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Detect.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Metadata including timestamp and coordinate frame</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Detect.input_timestamp","title":"input_timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_timestamp: Time = default_field(Time)\n</code></pre> <p>Timestamp of the input data (e.g., from camera)</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Detect.model_time","title":"model_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_time: Time = default_field(Time)\n</code></pre> <p>Timestamp when the object was processed by the model</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Detect.output_time","title":"output_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_time: Time = default_field(Time)\n</code></pre> <p>Timestamp when the processed output was available</p>"},{"location":"middleware/api/edgefirst_msgs/#dmabuffer","title":"DmaBuffer","text":""},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.fd","title":"fd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fd: int32 = 0\n</code></pre> <p>The file descriptor of the DMA buffer</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.fourcc","title":"fourcc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fourcc: uint32 = 0\n</code></pre> <p>The fourcc code of the image</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Metadata including timestamp and coordinate frame</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.height","title":"height  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>height: uint32 = 0\n</code></pre> <p>The height of the image in pixels</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.length","title":"length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>length: uint32 = 0\n</code></pre> <p>The length of the DMA buffer in bytes, used to mmap the buffer</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.pid","title":"pid  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pid: uint32 = 0\n</code></pre> <p>The process id of the service that created the DMA buffer</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.stride","title":"stride  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stride: uint32 = 0\n</code></pre> <p>The stride of the image in bytes</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.DmaBuffer.width","title":"width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>width: uint32 = 0\n</code></pre> <p>The width of the image in pixels</p>"},{"location":"middleware/api/edgefirst_msgs/#mask","title":"Mask","text":""},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Mask.encoding","title":"encoding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>encoding: str = ''\n</code></pre> <p>The optional encoding for the mask (currently unused).</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Mask.height","title":"height  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>height: uint32 = 0\n</code></pre> <p>The height of the mask, 0 if this dimension is unused.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Mask.length","title":"length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>length: uint32 = 0\n</code></pre> <p>The length of the mask, 0 if this dimension is unused.  The length would be used in 3D masks to represent the depth.  It could also be used for 2D bird's eye view masks along with width instead of height (elevation).</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Mask.mask","title":"mask  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask: sequence[uint8] = default_field([])\n</code></pre> <p>The segmentation mask data.  The array should be reshaped according to the height, width, and length dimensions.  The dimension order is row-major.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Mask.width","title":"width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>width: uint32 = 0\n</code></pre> <p>The width of the mask, 0 if this dimension is unused.</p>"},{"location":"middleware/api/edgefirst_msgs/#model","title":"Model","text":""},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.boxes","title":"boxes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>boxes: sequence[Box] = default_field([])\n</code></pre> <p>Array of detected object bounding boxes.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.decode_time","title":"decode_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>decode_time: Duration = Duration()\n</code></pre> <p>Duration to decode the outputs from the model, including nms and tracking.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = Header()\n</code></pre> <p>Metadata including timestamp and coordinate frame</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.input_time","title":"input_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_time: Duration = Duration()\n</code></pre> <p>Duration to load inputs into the model</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.mask","title":"mask  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask: sequence[Mask] = default_field([])\n</code></pre> <p>Segmentation masks from the model.  Empty array if model does not generate masks.  Generally models will only generate a single mask if they do.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.model_time","title":"model_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_time: Duration = Duration()\n</code></pre> <p>Duration to run the model, not including input/output/decoding</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Model.output_time","title":"output_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_time: Duration = Duration()\n</code></pre> <p>Duration to read outputs from the model</p>"},{"location":"middleware/api/edgefirst_msgs/#radarcube","title":"RadarCube","text":"<p>The RadarCube interface carries various radar cube reprensentations of the Radar FFT before generally being processed by CFAR into a point cloud.  The cube coud be R, RD, RAD, RA, and so on where R=Range, D=Dopper, and A=Azimuth.</p> <p>Dimensional labels are used to describe the radar cube layout.  Not all cubes include every label.  Undefined is used for dimensions not covered by this list.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.cube","title":"cube  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cube: sequence[int16] = default_field([])\n</code></pre> <p>The radar cube data as 16bit integers.  If the is_complex is true then each element will be pairs of integers with the first being real and the second being imaginary.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Message header containing the timestamp and frame id.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.is_complex","title":"is_complex  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_complex: bool = False\n</code></pre> <p>True if the radar cube is complex in which case the final dimension will be doubled in size to account for the pair of int16 elements representing [real,imaginary].</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.layout","title":"layout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>layout: sequence[uint8] = default_field([])\n</code></pre> <p>Radar cube layout provides labels for each dimensions</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.scales","title":"scales  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scales: sequence[float32] = default_field([])\n</code></pre> <p>The scaling factors for the dimensions representing bins.  For dimensions taken \"as-is\" the scale will be 1.0.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.shape","title":"shape  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>shape: sequence[uint16] = default_field([])\n</code></pre> <p>Radar cube shape provides the shape of each dimensions</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarCube.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: uint64 = 0\n</code></pre> <p>Radar frame timestamp generated on the radar module</p>"},{"location":"middleware/api/edgefirst_msgs/#radarinfo","title":"RadarInfo","text":"<p>The RadarInfo interface carries the current radar configuration and status.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarInfo.center_frequency","title":"center_frequency  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center_frequency: str = ''\n</code></pre> <p>Radar center frequency band.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarInfo.cube","title":"cube  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cube: bool = False\n</code></pre> <p>True if the radar is configured to output radar cubes.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarInfo.detection_sensitivity","title":"detection_sensitivity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>detection_sensitivity: str = ''\n</code></pre> <p>The detection sensitivity controls the sensitivity to recognize a target.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarInfo.frequency_sweep","title":"frequency_sweep  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frequency_sweep: str = ''\n</code></pre> <p>The frequency sweep controls the detection range of the radar.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarInfo.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = Header()\n</code></pre> <p>Message header containing the timestamp and frame id.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.RadarInfo.range_toggle","title":"range_toggle  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>range_toggle: str = ''\n</code></pre> <p>The range-toggle mode allows the radar to alternate between various frequency sweep configurations.  Applications must handle range toggling as targets are not consistent between messages as the frequency alternates.</p>"},{"location":"middleware/api/edgefirst_msgs/#track","title":"Track","text":""},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Track.created","title":"created  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created: Time = default_field(Time)\n</code></pre> <p>Time the track was first added</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Track.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = ''\n</code></pre> <p>Unique identifier for the object track, empty if the object is not tracked.</p>"},{"location":"middleware/api/edgefirst_msgs/#edgefirst.schemas.edgefirst_msgs.Track.lifetime","title":"lifetime  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lifetime: int32 = 0\n</code></pre> <p>Number of consecutive frames the object has been tracked</p>"},{"location":"middleware/api/foxglove_msgs/","title":"Foxglove Messages","text":""},{"location":"middleware/api/foxglove_msgs/#compressedimage","title":"CompressedImage","text":"<p>foxglove_msgs/msg/CompressedImage A compressed image</p> <p>Generated by https://github.com/foxglove/schemas</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedImage.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: sequence[uint8] = default_field([])\n</code></pre> <p>Compressed image data</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedImage.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format: str = ''\n</code></pre> <p>Image format</p> <p>Supported values: image media types supported by Chrome, such as <code>webp</code>, <code>jpeg</code>, <code>png</code></p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedImage.frame_id","title":"frame_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frame_id: str = ''\n</code></pre> <p>Frame of reference for the image. The origin of the frame is the optical center of the camera. +x points to the right in the image, +y points down, and +z points into the plane of the image.</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedImage.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: Time = default_field(Time)\n</code></pre> <p>Timestamp of image</p>"},{"location":"middleware/api/foxglove_msgs/#compressedvideo","title":"CompressedVideo","text":"<p>foxglove_msgs/msg/CompressedVideo A single frame of a compressed video bitstream</p> <p>Generated by https://github.com/foxglove/schemas</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedVideo.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: sequence[uint8] = default_field([])\n</code></pre> <p>Compressed video frame data.</p> <p>For packet-based video codecs this data must begin and end on packet boundaries (no partial packets), and must contain enough video packets to decode exactly one image (either a keyframe or delta frame). Note: Foxglove does not support video streams that include B frames because they require lookahead.</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedVideo.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format: str = ''\n</code></pre> <p>Video format.</p> <p>Supported values: <code>h264</code> (Annex B formatted data only)</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedVideo.frame_id","title":"frame_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frame_id: str = ''\n</code></pre> <p>Frame of reference for the video.</p> <p>The origin of the frame is the optical center of the camera. +x points to the right in the video, +y points down, and +z points into the plane of the video.</p>"},{"location":"middleware/api/foxglove_msgs/#edgefirst.schemas.foxglove_msgs.CompressedVideo.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: Time = default_field(Time)\n</code></pre> <p>Timestamp of video frame</p>"},{"location":"middleware/api/geometry_msgs/","title":"ROS2 Geometry Messages","text":""},{"location":"middleware/api/geometry_msgs/#quaternion","title":"Quaternion","text":"<p>This represents an orientation in free space in quaternion form.</p>"},{"location":"middleware/api/geometry_msgs/#vector3","title":"Vector3","text":"<p>This represents a vector in free space. This is semantically different than a point. A vector is always anchored at the origin. When a transform is applied to a vector, only the rotational component is applied.</p>"},{"location":"middleware/api/geometry_msgs/#accel","title":"Accel","text":"<p>This expresses acceleration in free space broken into its linear and angular parts.</p>"},{"location":"middleware/api/geometry_msgs/#accelwithcovariance","title":"AccelWithCovariance","text":"<p>This expresses acceleration in free space with uncertainty.</p>"},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.AccelWithCovariance.covariance","title":"covariance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>covariance: array[float64, 36] = default_field([0] * 36)\n</code></pre> <p>Row-major representation of the 6x6 covariance matrix The orientation parameters use a fixed-axis representation. In order, the parameters are: (x, y, z, rotation about X axis, rotation about Y axis, rotation about Z axis)</p>"},{"location":"middleware/api/geometry_msgs/#inertia","title":"Inertia","text":""},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.Inertia.com","title":"com  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>com: Vector3 = default_field(Vector3)\n</code></pre> <p>Center of mass [m]</p>"},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.Inertia.izz","title":"izz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>izz: float64 = 0\n</code></pre> <p>Inertia Tensor [kg-m^2]     | ixx ixy ixz | I = | ixy iyy iyz |     | ixz iyz izz |</p>"},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.Inertia.m","title":"m  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>m: float64 = 0\n</code></pre> <p>Mass [kg]</p>"},{"location":"middleware/api/geometry_msgs/#point","title":"Point","text":"<p>This contains the position of a point in free space</p>"},{"location":"middleware/api/geometry_msgs/#point32","title":"Point32","text":"<p>This contains the position of a point in free space(with 32 bits of precision). It is recommended to use Point wherever possible instead of Point32. This recommendation is to promote interoperability. This message is designed to take up less space when sending lots of points at once, as in the case of a PointCloud.</p>"},{"location":"middleware/api/geometry_msgs/#polygon","title":"Polygon","text":"<p>A specification of a polygon where the first and last points are assumed to be connected</p>"},{"location":"middleware/api/geometry_msgs/#pose","title":"Pose","text":"<p>A representation of pose in free space, composed of position and orientation.</p>"},{"location":"middleware/api/geometry_msgs/#transform","title":"Transform","text":"<p>This represents the transform between two coordinate frames in free space.</p>"},{"location":"middleware/api/geometry_msgs/#transformstamped","title":"TransformStamped","text":"<p>This expresses a transform from coordinate frame header.frame_id to the coordinate frame child_frame_id at the time of header.stamp</p> <p>This message is mostly used by the tf2 package. See its documentation for more information.</p> <p>The child_frame_id is necessary in addition to the frame_id in the Header to communicate the full reference for the transform in a self contained message.</p>"},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.TransformStamped.child_frame_id","title":"child_frame_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>child_frame_id: str = ''\n</code></pre> <p>The frame id of the child frame to which this transform points.</p>"},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.TransformStamped.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>The frame id in the header is used as the reference frame of this transform.</p>"},{"location":"middleware/api/geometry_msgs/#edgefirst.schemas.geometry_msgs.TransformStamped.transform","title":"transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transform: Transform = default_field(Transform)\n</code></pre> <p>Translation and rotation in 3-dimensions of child_frame_id from header.frame_id.</p>"},{"location":"middleware/api/nav_msgs/","title":"ROS2 Navigation Messages","text":""},{"location":"middleware/api/nav_msgs/#gridcells","title":"GridCells","text":"<p>An array of cells in a 2D grid</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.GridCells.cell_height","title":"cell_height  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cell_height: float32 = 0\n</code></pre> <p>Height of each cell</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.GridCells.cell_width","title":"cell_width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cell_width: float32 = 0\n</code></pre> <p>Width of each cell</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.GridCells.cells","title":"cells  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cells: sequence[Point] = default_field([])\n</code></pre> <p>Each cell is represented by the Point at the center of the cell</p>"},{"location":"middleware/api/nav_msgs/#mapmetadata","title":"MapMetaData","text":"<p>This hold basic information about the characteristics of the OccupancyGrid</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.MapMetaData.height","title":"height  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>height: uint32 = 0\n</code></pre> <p>Map height [cells]</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.MapMetaData.map_load_time","title":"map_load_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>map_load_time: Time = default_field(Time)\n</code></pre> <p>The time at which the map was loaded</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.MapMetaData.origin","title":"origin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>origin: Pose = default_field(Pose)\n</code></pre> <p>The origin of the map [m, m, rad].  This is the real-world pose of the bottom left corner of cell (0,0) in the map.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.MapMetaData.resolution","title":"resolution  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resolution: float32 = 0\n</code></pre> <p>The map resolution [m/cell]</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.MapMetaData.width","title":"width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>width: uint32 = 0\n</code></pre> <p>Map width [cells]</p>"},{"location":"middleware/api/nav_msgs/#occupancygrid","title":"OccupancyGrid","text":"<p>This represents a 2-D grid map</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.OccupancyGrid.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: sequence[int8] = default_field([])\n</code></pre> <p>The map data, in row-major order, starting with (0,0).  Cell (1, 0) will be listed second, representing the next cell in the x direction.  Cell (0, 1) will be at the index equal to info.width, followed by (1, 1). The values inside are application dependent, but frequently,  0 represents unoccupied, 1 represents definitely occupied, and -1 represents unknown.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.OccupancyGrid.info","title":"info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>info: MapMetaData = default_field(MapMetaData)\n</code></pre> <p>MetaData for the map</p>"},{"location":"middleware/api/nav_msgs/#odometry","title":"Odometry","text":"<p>This represents an estimate of a position and velocity in free space. The pose in this message should be specified in the coordinate frame given by header.frame_id The twist in this message should be specified in the coordinate frame given by the child_frame_id</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.Odometry.child_frame_id","title":"child_frame_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>child_frame_id: str = ''\n</code></pre> <p>Frame id the pose points to. The twist is in this coordinate frame.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.Odometry.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Includes the frame id of the pose parent.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.Odometry.pose","title":"pose  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pose: PoseWithCovariance = default_field(PoseWithCovariance)\n</code></pre> <p>Estimated pose that is typically relative to a fixed world frame.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.Odometry.twist","title":"twist  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>twist: TwistWithCovariance = default_field(TwistWithCovariance)\n</code></pre> <p>Estimated linear and angular velocity relative to child_frame_id.</p>"},{"location":"middleware/api/nav_msgs/#path","title":"Path","text":"<p>An array of poses that represents a Path for a robot to follow.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.Path.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Indicates the frame_id of the path.</p>"},{"location":"middleware/api/nav_msgs/#edgefirst.schemas.nav_msgs.Path.poses","title":"poses  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>poses: sequence[PoseStamped] = default_field([])\n</code></pre> <p>Array of poses to follow.</p>"},{"location":"middleware/api/sensor_msgs/","title":"ROS2 Sensor Messages","text":""},{"location":"middleware/api/sensor_msgs/#camerainfo","title":"CameraInfo","text":"<p>This message defines meta information for a camera. It should be in a camera namespace on topic \"camera_info\" and accompanied by up to five image topics named:</p> <ul> <li>image_raw: raw data from the camera driver, possibly Bayer encoded</li> <li>image: monochrome, distorted</li> <li>image_color: color, distorted</li> <li>image_rect: monochrome, rectified</li> <li>image_rect_color: color, rectified</li> </ul> <p>The image_pipeline contains packages (image_proc, stereo_image_proc) for producing the four processed image topics from image_raw and camera_info. The meaning of the camera parameters are described in detail at http://www.ros.org/wiki/image_pipeline/CameraInfo.</p> <p>The image_geometry package provides a user-friendly interface to common operations using this meta information. If you want to, e.g., project a 3d point into image coordinates, we strongly recommend using image_geometry.</p> <p>If the camera is uncalibrated, the matrices D, K, R, P should be left zeroed out. In particular, clients may assume that K[0] == 0.0 indicates an uncalibrated camera.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.d","title":"d  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>d: sequence[float64] = default_field([])\n</code></pre> <p>The distortion parameters, size depending on the distortion model. For \"plumb_bob\", the 5 parameters are: (k1, k2, t1, t2, k3).</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.distortion_model","title":"distortion_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distortion_model: str = ''\n</code></pre> <p>The distortion model used. Supported models are listed in sensor_msgs/distortion_models.hpp. For most cameras, \"plumb_bob\" - a simple model of radial and tangential distortion - is sufficent.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Time of image acquisition, camera coordinate frame ID Header timestamp should be acquisition time of image Header frame_id should be optical frame of camera origin of frame should be optical center of camera +x should point to the right in the image +y should point down in the image +z should point into the plane of the image</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.k","title":"k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>k: array[float64, 9] = default_field([0] * 9)\n</code></pre> <p>3x3 row-major matrix Intrinsic camera matrix for the raw (distorted) images.     [fx  0 cx] K = [ 0 fy cy]     [ 0  0  1] Projects 3D points in the camera coordinate frame to 2D pixel coordinates using the focal lengths (fx, fy) and principal point (cx, cy).</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.p","title":"p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>p: array[float64, 12] = default_field([0] * 12)\n</code></pre> <p>3x4 row-major matrix Projection/camera matrix     [fx'  0  cx' Tx] P = [ 0  fy' cy' Ty]     [ 0   0   1   0] By convention, this matrix specifies the intrinsic (camera) matrix  of the processed (rectified) image. That is, the left 3x3 portion  is the normal camera intrinsic matrix for the rectified image. It projects 3D points in the camera coordinate frame to 2D pixel  coordinates using the focal lengths (fx', fy') and principal point  (cx', cy') - these may differ from the values in K. For monocular cameras, Tx = Ty = 0. Normally, monocular cameras will  also have R = the identity and P[1:3,1:3] = K. For a stereo pair, the fourth column [Tx Ty 0]' is related to the  position of the optical center of the second camera in the first  camera's frame. We assume Tz = 0 so both cameras are in the same  stereo image plane. The first camera always has Tx = Ty = 0. For  the right (second) camera of a horizontal stereo pair, Ty = 0 and  Tx = -fx' * B, where B is the baseline between the cameras. Given a 3D point [X Y Z]', the projection (x, y) of the point onto  the rectified image is given by:  [u v w]' = P * [X Y Z 1]'         x = u / w         y = v / w  This holds for both images of a stereo pair.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.r","title":"r  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>r: array[float64, 9] = default_field([0] * 9)\n</code></pre> <p>3x3 row-major matrix Rectification matrix (stereo cameras only) A rotation matrix aligning the camera coordinate system to the ideal stereo image plane so that epipolar lines in both stereo images are parallel.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CameraInfo.roi","title":"roi  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>roi: RegionOfInterest = default_field(RegionOfInterest)\n</code></pre> <p>Region of interest (subwindow of full camera resolution), given in  full resolution (unbinned) image coordinates. A particular ROI  always denotes the same window of pixels on the camera sensor,  regardless of binning settings. The default setting of roi (all values 0) is considered the same as  full resolution (roi.width = width, roi.height = height).</p>"},{"location":"middleware/api/sensor_msgs/#compressedimage","title":"CompressedImage","text":"<p>This message contains a compressed image.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CompressedImage.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: sequence[uint8] = default_field([])\n</code></pre> <p>Compressed image buffer</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CompressedImage.format","title":"format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>format: str = ''\n</code></pre> <p>Specifies the format of the data   Acceptable values:     jpeg, png, tiff</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.CompressedImage.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Header timestamp should be acquisition time of image Header frame_id should be optical frame of camera origin of frame should be optical center of cameara +x should point to the right in the image +y should point down in the image +z should point into to plane of the image</p>"},{"location":"middleware/api/sensor_msgs/#image","title":"Image","text":"<p>This message contains an uncompressed image (0, 0) is at top-left corner of image</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: sequence[uint8] = default_field([])\n</code></pre> <p>actual matrix data, size is (step * rows)</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.encoding","title":"encoding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>encoding: str = ''\n</code></pre> <p>The legal values for encoding are in file src/image_encodings.cpp If you want to standardize a new string format, join ros-users@lists.ros.org and send an email proposing a new encoding. Encoding of pixels -- channel meaning, ordering, size taken from the list of strings in include/sensor_msgs/image_encodings.hpp</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Header timestamp should be acquisition time of image Header frame_id should be optical frame of camera origin of frame should be optical center of cameara +x should point to the right in the image +y should point down in the image +z should point into to plane of the image If the frame_id here and the frame_id of the CameraInfo message associated with the image conflict the behavior is undefined</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.height","title":"height  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>height: uint32 = 0\n</code></pre> <p>image height, that is, number of rows</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.is_bigendian","title":"is_bigendian  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_bigendian: uint8 = 0\n</code></pre> <p>is this data bigendian?</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.step","title":"step  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>step: uint32 = 0\n</code></pre> <p>Full row length in bytes</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Image.width","title":"width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>width: uint32 = 0\n</code></pre> <p>image width, that is, number of columns</p>"},{"location":"middleware/api/sensor_msgs/#imu","title":"Imu","text":"<p>This is a message to hold data from an IMU (Inertial Measurement Unit)</p> <p>Accelerations should be in m/s^2 (not in g's), and rotational velocity should be in rad/sec</p> <p>If the covariance of the measurement is known, it should be filled in (if all you know is the variance of each measurement, e.g. from the datasheet, just put those along the diagonal) A covariance matrix of all zeros will be interpreted as \"covariance unknown\", and to use the data a covariance will have to be assumed or gotten from some other source</p> <p>If you have no estimate for one of the data elements (e.g. your IMU doesn't produce an orientation estimate), please set element 0 of the associated covariance matrix to -1 If you are interpreting this message, please check for a value of -1 in the first element of each covariance matrix, and disregard the associated estimate.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Imu.angular_velocity_covariance","title":"angular_velocity_covariance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>angular_velocity_covariance: array[float64, 9] = default_field([0] * 9)\n</code></pre> <p>Row major about x, y, z axes</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Imu.linear_acceleration_covariance","title":"linear_acceleration_covariance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linear_acceleration_covariance: array[float64, 9] = default_field([0] * 9)\n</code></pre> <p>Row major x, y z</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Imu.orientation_covariance","title":"orientation_covariance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orientation_covariance: array[float64, 9] = default_field([0] * 9)\n</code></pre> <p>Row major about x, y, z axes</p>"},{"location":"middleware/api/sensor_msgs/#navsatstatus","title":"NavSatStatus","text":"<p>Navigation Satellite fix status for any Global Navigation Satellite System.</p> <p>Whether to output an augmented fix is determined by both the fix type and the last time differential corrections were received.  A fix is valid when status &gt;= STATUS_FIX.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatStatus.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: int8 = 0\n</code></pre> <p>Bits defining which Global Navigation Satellite System signals were used by the receiver.</p>"},{"location":"middleware/api/sensor_msgs/#navsatfix","title":"NavSatFix","text":"<p>Navigation Satellite fix for any Global Navigation Satellite System</p> <p>Specified using the WGS 84 reference ellipsoid</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.altitude","title":"altitude  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>altitude: float64 = 0\n</code></pre> <p>Altitude [m]. Positive is above the WGS 84 ellipsoid (quiet NaN if no altitude is available).</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>header.stamp specifies the ROS time for this measurement (the        corresponding satellite time may be reported using the        sensor_msgs/TimeReference message).</p> <p>header.frame_id is the frame of reference reported by the satellite        receiver, usually the location of the antenna.  This is a        Euclidean frame relative to the vehicle, not a reference        ellipsoid.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.latitude","title":"latitude  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>latitude: float64 = 0\n</code></pre> <p>Latitude [degrees]. Positive is north of equator; negative is south.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.longitude","title":"longitude  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>longitude: float64 = 0\n</code></pre> <p>Longitude [degrees]. Positive is east of prime meridian; negative is west.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.position_covariance","title":"position_covariance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>position_covariance: array[float64, 9] = default_field([0] * 9)\n</code></pre> <p>Position covariance [m^2] defined relative to a tangential plane through the reported position. The components are East, North, and Up (ENU), in row-major order.</p> <p>Beware: this coordinate system exhibits singularities at the poles.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.position_covariance_type","title":"position_covariance_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>position_covariance_type: uint8 = 0\n</code></pre> <p>If the covariance of the fix is known, fill it in completely. If the GPS receiver provides the variance of each measurement, put them along the diagonal. If only Dilution of Precision is available, estimate an approximate covariance from that.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.NavSatFix.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: NavSatStatus = default_field(NavSatStatus)\n</code></pre> <p>Satellite fix status information.</p>"},{"location":"middleware/api/sensor_msgs/#pointfield","title":"PointField","text":"<p>This message holds the description of one point entry in the PointCloud2 message format.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointField.count","title":"count  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>count: uint32 = 0\n</code></pre> <p>How many elements in the field</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointField.datatype","title":"datatype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datatype: uint8 = 0\n</code></pre> <p>Datatype enumeration, see above</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointField.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = ''\n</code></pre> <p>Name of field Common PointField names are x, y, z, intensity, rgb, rgba</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointField.offset","title":"offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset: uint32 = 0\n</code></pre> <p>Offset from start of point struct</p>"},{"location":"middleware/api/sensor_msgs/#pointcloud2","title":"PointCloud2","text":"<p>This message holds a collection of N-dimensional points, which may contain additional information such as normals, intensity, etc. The point data is stored as a binary blob, its layout described by the contents of the \"fields\" array.</p> <p>The point cloud data may be organized 2d (image-like) or 1d (unordered). Point clouds organized as 2d images may be produced by camera depth sensors such as stereo or time-of-flight.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: sequence[uint8] = default_field([])\n</code></pre> <p>Actual point data, size is (row_step*height)</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.fields","title":"fields  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fields: sequence[PointField] = default_field([])\n</code></pre> <p>Describes the channels and their layout in the binary data blob.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>Time of sensor data acquisition, and the coordinate frame ID (for 3d points).</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.is_bigendian","title":"is_bigendian  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_bigendian: bool = False\n</code></pre> <p>Is this data bigendian?</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.is_dense","title":"is_dense  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_dense: bool = False\n</code></pre> <p>True if there are no invalid points</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.point_step","title":"point_step  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>point_step: uint32 = 0\n</code></pre> <p>Length of a point in bytes</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.PointCloud2.row_step","title":"row_step  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>row_step: uint32 = 0\n</code></pre> <p>Length of a row in bytes</p>"},{"location":"middleware/api/sensor_msgs/#relativehumidity","title":"RelativeHumidity","text":"<p>Single reading from a relative humidity sensor. Defines the ratio of partial pressure of water vapor to the saturated vapor pressure at a temperature.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.RelativeHumidity.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>timestamp of the measurement frame_id is the location of the humidity sensor</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.RelativeHumidity.relative_humidity","title":"relative_humidity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>relative_humidity: float64 = 0\n</code></pre> <p>Expression of the relative humidity from 0.0 to 1.0. 0.0 is no partial pressure of water vapor 1.0 represents partial pressure of saturation</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.RelativeHumidity.variance","title":"variance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variance: float64 = 0\n</code></pre> <p>0 is interpreted as variance unknown</p>"},{"location":"middleware/api/sensor_msgs/#temperature","title":"Temperature","text":"<p>Single temperature reading.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Temperature.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>timestamp is the time the temperature was measured frame_id is the location of the temperature reading</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Temperature.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float64 = 0\n</code></pre> <p>Measurement of the Temperature in Degrees Celsius.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.Temperature.variance","title":"variance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variance: float64 = 0\n</code></pre> <p>0 is interpreted as variance unknown.</p>"},{"location":"middleware/api/sensor_msgs/#timereference","title":"TimeReference","text":"<p>Measurement from an external time source not actively synchronized with the system clock.</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.TimeReference.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: Header = default_field(Header)\n</code></pre> <p>stamp is system time for which measurement was valid frame_id is not used</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.TimeReference.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: str = ''\n</code></pre> <p>(optional) name of time source</p>"},{"location":"middleware/api/sensor_msgs/#edgefirst.schemas.sensor_msgs.TimeReference.time_ref","title":"time_ref  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>time_ref: Time = default_field(Time)\n</code></pre> <p>corresponding time from this external source</p>"},{"location":"middleware/api/std_msgs/","title":"ROS2 Standard Messages","text":""},{"location":"middleware/api/std_msgs/#colorrgba","title":"ColorRGBA","text":""},{"location":"middleware/api/std_msgs/#header","title":"Header","text":"<p>Standard metadata for higher-level stamped data types. This is generally used to communicate timestamped data in a particular coordinate frame.</p>"},{"location":"middleware/api/std_msgs/#edgefirst.schemas.std_msgs.Header.frame_id","title":"frame_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frame_id: str = ''\n</code></pre> <p>Transform frame with which this data is associated.</p>"},{"location":"middleware/api/std_msgs/#edgefirst.schemas.std_msgs.Header.stamp","title":"stamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stamp: Time = default_field(Time)\n</code></pre> <p>Two-integer timestamp that is expressed as seconds and nanoseconds.</p>"},{"location":"models/","title":"EdgeFirst Models","text":""},{"location":"models/augmentations/","title":"Vision Augmentations","text":"<p>This page is dedicated to describing all available vision augmentations during training and model validation. For training purposes, vision augmentations are techniques used to increase the number of training samples in the dataset by providing more variations for each sample thus exposing the model with samples that have a diverse set of characteristics and conditions. For validation purposes, the model performance is evaluated for cases where there is a level of synthetic degradation imposed on the camera to mimic environmental conditions such as heavy rain, fog, snow, etc. where camera-based models could fail at detecting objects in the scene, whereas models with radar-based aspects could be more robust in such conditions. </p> <p>Note: For Fusion models, the camera is an optional sensor, but for ModelPack the camera is a requirement since ModelPack is a single-sensor (single-input) architecture and Fusion is a multi-sensor (multi-input) architecture.</p> <p>The camera augmentations are distinguished into two categories: Geometric and Non-Geometric.</p>"},{"location":"models/augmentations/#geometric","title":"Geometric","text":"<p>These augmentations are based on transformations of the spatial orientation of the image. For these type of augmentations, the ground truth annotations needs to adhere to the changes in the orientation of the objects in the image aswell. The augmentations below provide visualizations for demonstrating the image and ground truth transformation.</p>"},{"location":"models/augmentations/#shift","title":"Shift","text":"<p>This type of augmentation translates the image in either x, y, or both axis causing certain portions of the image to disappear, but allowing grayed out areas in order to compensate for the translation maintaining the original image resolution. In these examples, the ground truth mask and bounding box annotation is maintained around the person after the augmentations. </p> Without Shift Negative XY Shift Positive XY shift"},{"location":"models/augmentations/#scale","title":"Scale","text":"<p>This type of augmentation resizes the actual image either smaller or larger, but maintaining the original image resolution. For the example of image downscaling in the middle, the actual image is positioned on the top left corner, but grayed out areas are added to maintain the original image resolution. For the example of image upscaling on the right, the image has the effect of being zoomed in, thus leaving out certain portions of the image. However, for both types of augmentations, the ground truth annotation around the person is maintained.</p> Original Scaling 0.50 Scaling 1.50 Scaling"},{"location":"models/augmentations/#hflip","title":"HFlip","text":"<p>This type of augmentation flips the image in the x-axis. Once the image is flipped, the ground truth annotations are also flipped to ensure the annotations properly represent the objects in the scene. In this example, the mask and the bounding box around the person are also flipped.</p> Original Horizontal Flip <p>Note: The augmentation for a vertical flip is omitted because these cases are arguably rare for samples being upside-down.</p>"},{"location":"models/augmentations/#non-geometric","title":"Non-Geometric","text":"<p>These augmentations are based on transformations of the color properties or applying various filters on the image to modify visual characteristics of the image. The main intention behind these augmentations is to provide practical visual alterations just enough to maintain a level of realistic representations of the scene. Heavier alterations are primarily due to blur and solarizations which are still reproducible in real-world environments such as heavy fog or extreme sun glare.</p>"},{"location":"models/augmentations/#blur","title":"Blur","text":"<p>This augmentation reduces the clarity of the image with levels from 0-100% where 0% indicates no blur applied to the image and 100% indicates maximum blur rendering the image visually opaque. The effects of blur in levels of 0%, 20%, 50%, 80%, 100% are shown below.</p> 0% Blur 20% Blur 50% Blur 80% Blur 100% Blur"},{"location":"models/augmentations/#grayscale","title":"Grayscale","text":"<p>This augmentation converts a color image to a grayscale image. A grayscale image is a single channel image where each pixel represents light intensity. </p> <p>Note: This augmentation is not exposed as an option in DVE because its transformation has no other variations, but model training utilizes this augmentation as part of the random selection of augmentations.</p> Color Image Grayscale Image"},{"location":"models/augmentations/#solarize","title":"Solarize","text":"<p>This augmentation manipulates the distribution of light and dark pixels based on a given threshold. The darker pixels (below the threshold) to remain the same. The brighter pixels (above the threshold) to be inverted, creating a high-contrast effect. The following side-by-side figures shows the original image on the left and the transformed image with solarization on the right given a threshold of 128 for maximum effect.</p> Without Solarization With Solarization"},{"location":"models/augmentations/#contrast","title":"Contrast","text":"<p>This augmentation enhances the contrast in the image by enhancing the difference between color in different parts of the image. It computes the contrast by shifting each pixel relative to the mean pixel value of the image and then scaling it by the contrast factor. The following side-by-side figures shows the original image on the left and the transformed image on the right with a contrast value of 150 for maximum effect.</p> Without Contrast With Contrast"},{"location":"models/augmentations/#brightness","title":"Brightness","text":"<p>This augmentation either dims or brightens the image by scaling the values in all channels of the image by a fixed factor. The following side-by-side figures shows the original image on the left and the transformed image with a brightness of 50 in the center and a brightness of 200 on the right for maximum effect.</p> Original Brightness Brightness of 50 Brightness of 200"},{"location":"models/augmentations/#hue","title":"Hue","text":"<p>This augmentation shifts the color of the image along the HSV color space. A parameter is provided to determine the extent of the hue adjustment. As an example, a red color might become blue or green depending on the hue adjustment. The following side-by-side figures shows the original image on the left and the transformed image on the right with hue set to 100 for maximum effect.</p> Original With Hue Adjustments"},{"location":"models/augmentations/#saturation","title":"Saturation","text":"<p>This augmentation applies saturation in the HSV (Hue, Saturation, Value) color space. This augmentation either increases the intensity of the colors in the image for increased vibrancy or a decrease in intensity to become much more grayscale or monotone. The following side-by-side figures shows the original image on the left and the transformed image on the right with saturation set to 150 for maximum effect.</p> Original With Saturation Adjustments"},{"location":"models/augmentations/#plasma","title":"Plasma","text":"<p>This augmentation applies a random plasma texture on the image using a Gaussian-filtered noise to enhance the contrast. The effect will result in an image with random variations (perturbations) to the pixel values in both bright and dark areas. The following side-by-side figures shows the original image on the left and the transformed image on the right with a plasma texture applied. </p> Original With Plasma Adjustments"},{"location":"models/augmentations/#random-patch","title":"Random Patch","text":"<p>This augmentation generates a random patch within the image space. Then runs a random augmentation from the augmentations described above on that patch. The following side-by-side figures shows the original image on the left and the transformed image on the right with a random patch augmentation. </p> Original With Patch Adjustments"},{"location":"models/model/training/fusion/","title":"Fusion","text":"<p>This page will provide a walk-through on using DVE for training Fusion models.</p> <ol> <li> <p>Select Trainer from the tool options. </p> <p> Tool Options </p> </li> <li> <p>Specify the project to run training at the center of the top menu bar.</p> <p> Project Selection </p> </li> <li> <p>Create a new training session by clicking the create button on the top right.</p> <p> Create New Session </p> </li> <li> <p>Configure the settings on the left panel by specifying Trainer Type to EdgeFirst Fusion and provide additional configurations for the name of the session and the dataset to deploy. Next configure the settings on the right panel by specifying training parameters. By default the Fusion model is configured with both camera and radar inputs, however, a Camera-Only or Radar-Only model are possible variations. </p> <p>Note: Additional information on these parameters are provided by hovering over the info button </p> <p>Note: For more information on available vision augmentations please see Vision Augmentations.</p> <p> Training Options </p> </li> <li> <p>Start the session by clicking the START SESSION button on the bottom right.</p> <p> Start the Session </p> </li> <li> <p>The training session has now started while the progress is tracked on the left panel and additional information and status is shown on the right panel.</p> <p> Training Session </p> </li> <li> <p>Once completed, the status will be shown as complete.</p> <p> Completed Session </p> </li> <li> <p>The training metrics are shown by clicking the plots button on the top left of the session card. </p> <p> Training Metrics </p> </li> <li> <p>The trained Keras and TFLite models can be found and downloaded by clicking on the maximize button next to the plots button on the top right of the session card. This will open a new dialog with the session details and the models are placed on the top right which can then be downloaded.</p> <p> Session Details </p> </li> <li> <p>Now you have generated your Fusion model, follow these next steps for validating your Fusion model.</p> </li> </ol>"},{"location":"models/model/training/modelpack/","title":"Modelpack","text":"<p>This page will provide a walk-through on using DVE for training Vision models using Modelpack.</p> <ol> <li> <p>Select Trainer from the tool options. </p> <p> Tool Options </p> </li> <li> <p>Specify the project to run training at the center of the top menu bar.</p> <p> Project Selection </p> </li> <li> <p>Create a new training session by clicking the create button on the top right.</p> <p> Create New Session </p> </li> <li> <p>Configure the settings on the left panel by specifying Trainer Type to Modelpack and provide additional configurations for the name of the session and the dataset to deploy. Next configure the settings on the right panel by specifying training parameters. By default a segmentation model will be trained, however, object detection or multi-task based models are possible variations. </p> <p>Note: Additional information on these parameters are provided by hovering over the info button. </p> <p>Note: For more information on available vision augmentation please see Vision Augmentations.</p> <p> Training Options </p> </li> <li> <p>Start the session by clicking the START SESSION button on the bottom right.</p> <p> Start the Session </p> </li> <li> <p>The training session has now started while the progress is tracked on the left panel and additional information and status is shown on the right panel.</p> <p> Training Session </p> </li> <li> <p>Once completed, the status will be shown as complete.</p> <p> Completed Session </p> </li> <li> <p>The training metrics are shown by clicking the plots button on the top left of the session card. </p> <p> Training Metrics </p> </li> <li> <p>The trained Keras, TFLite, and RTM models can be found and downloaded by clicking on the maximize button next to the plots button on the top right of the session card. This will open a new dialog with the session details and the models are placed on the top right which can then be downloaded.</p> <p> Session Details </p> </li> <li> <p>Now you have generated your Vision model, follow these next steps for validating your Vision model.</p> </li> </ol>"},{"location":"models/model/validation/fusion/","title":"Fusion","text":"<p>This page will provide a walk-through on using DVE for validating the performance of Fusion models that have been trained, through the QuickStart Guide or Training Fusion. This page will focus only on the validation of Fusion models.</p> <ol> <li> <p>Select Validator from the tool options.</p> <p> Tool Options </p> </li> <li> <p>Specify the project to run validation at the center of the top menu bar.</p> <p> Project Selection </p> </li> <li> <p>Create a new validation session by clicking the create button on the top right of the page.</p> <p> Create New Session </p> </li> <li> <p>Configure the settings on the left panel by specifying the name of the validation session, the model file to validate, and the dataset to deploy. Next configure the settings on the right panel by specifying the validation parameters. </p> <p>Note: Additional information on these parameters are provided by hovering over the info button. </p> <p>Note: The only augmentation available for this type of validation is <code>blur</code>. See Vision Augmentations for further details.</p> <p> Validation Options </p> </li> <li> <p>Start the session by clicking the START SESSION button on the bottom right.</p> <p> Start the Session </p> </li> <li> <p>The validation session has now started while the progress is tracked on the left panel and additional information and status is shown on the right panel.  </p> <p> Validation Session </p> </li> <li> <p>Once completed, the status will be shown as complete.</p> <p> Completed Session </p> </li> <li> <p>The metrics are shown by clicking the plots button on the top left of the session card.</p> <p> Validation Metrics </p> <p>The metrics provides the precision, recall, F1, and IoU scores of the model at the specified window sizes. Additional charts are provided for the precision vs. recall and bird\u2019s eye view heatmaps describing where the model performs well and where the model makes errors. </p> <p>Note: See Metrics for further details.</p> </li> <li> <p>It is also possible to compare validation metrics for multiple sessions. This is done by checking the checkboxes on the top left of the session cards.</p> <p> Comparing Sessions </p> <p>Compare the validation sessions by clicking the COMPARE VALIDATE SESSION button on the top left. This will display the validation metrics side by side for the specified validation sessions.</p> <p> Metrics Side-by-Side </p> </li> </ol>"},{"location":"models/model/validation/metrics/","title":"Metrics","text":"<p>This page is dedicated to describing the metrics reported in DVE after running validation. DVE supports validation of Modelpack and Fusion models. The following sections will describe the metrics reported for each type of validation.</p>"},{"location":"models/model/validation/metrics/#modelpack","title":"Modelpack","text":"<p>This section will describe the validation metrics reported in Modelpack validation sessions.</p>"},{"location":"models/model/validation/metrics/#object-detection-metrics","title":"Object Detection Metrics","text":"<p>The object detection metrics describe the mean average precision (mAP), recall (mAR), and accuracy (mACC) at IoU thresholds 0.50, 0.75, and 0.50-0.95. These metrics are represented as a bar chart. Shown below is an example.</p> Detection Metrics"},{"location":"models/model/validation/metrics/#mean-average-precision","title":"Mean Average Precision","text":"<p>The mAP is based on the area under the precision vs. recall curve which plots the tradeoff between precision and recall by adjusting the IoU thresholds. The average precision is first calculated by finding the area under the precision vs. recall curve for each class at varying IoU thresholds. The mAP at 0.50 and 0.75 is the mean of the average precision across all classes, but only at the IoU threshold values of 0.50 and 0.75. For the case of mAP at 0.50-0.95, the average precision at 0.50-0.95 is first calculated by taking the mean of the average precision (area under the curve) across IoU thresholds 0.50 to 0.95 in 0.05 steps. This process is done per class and the final mAP at 0.50-0.95 is the mean of the average precision at 0.50-0.95 values across all classes.</p>"},{"location":"models/model/validation/metrics/#mean-average-recall","title":"Mean Average Recall","text":"<p>This metric is calculated as the sum of the recall values of each class over the number of classes at specified IoU thresholds.  The mean average recall at IoU thresholds 0.50 and 0.75 are calculated based on the equations below. </p> \\[ \\text{mAR} = \\frac{1}{n}\\sum_{i=1}^{n}\\text{recall}_{i}, n = \\text{number of classes} \\] <p>Note: The equation for recall is shown in the Glossary.</p> <p>The metric for mAR 0.50-0.95 is calculated by taking the sum of mAR values at IoU thresholds 0.50, 0.55, ..., 0.95 and then dividing by the number of validation IoU thresholds (in this case 10). </p> \\[ \\text{mAR}_{0.50-0.95} = \\frac{1}{10}\\sum_{i=0.50}^{n}\\text{mAR}_{i}, i = \\text{0.50, 0.55, 0.60, ..., 0.95} \\]"},{"location":"models/model/validation/metrics/#mean-average-accuracy","title":"Mean Average Accuracy","text":"<p>This metric is calculated as the sum of accuracy values of each class over the number of classes at specified IoU thresholds. The mean average accuracy at IoU thresholds 0.50 and 0.75 are calculated based on the equations below. </p> \\[ \\text{mACC} = \\frac{1}{n}\\sum_{i=1}^{n}\\text{accuracy}_{i}, n = \\text{number of classes} \\] <p>Note: The equation for accuracy is shown in the Glossary.</p> <p>The following equation below calculates the mean average accuracy for a range of IoU thresholds from 0.50-0.95 which is calculated similarly to mean average recall. </p> \\[ \\text{mACC}_{0.50-0.95} = \\frac{1}{10}\\sum_{i=0.50}^{n}\\text{mACC}_{i}, i = \\text{0.50, 0.55, 0.60, ..., 0.95} \\]"},{"location":"models/model/validation/metrics/#segmentation-metrics","title":"Segmentation Metrics","text":"<p>The segmentation metrics describe precision, recall, accuracy, and IoU. These metrics are represented as a bar chart. Shown below is an example.</p> Segmentation Metrics"},{"location":"models/model/validation/metrics/#precision","title":"Precision","text":"<p>This metric is calculated based on the equation for precision shown in the Glossary. A true positive is when the prediction label matches the true (actual) label. A false positive is when the prediction label does not match the true (actual) label.</p>"},{"location":"models/model/validation/metrics/#recall","title":"Recall","text":"<p>This metric is calculated based on the equation for recall shown in the Glossary. A true positive is when the prediction label matches the true (actual) label. A false negative is when the prediction label is a positive class, but the true (actual) label is a negative class (background).</p>"},{"location":"models/model/validation/metrics/#accuracy","title":"Accuracy","text":"<p>This metric is based on the proportion of the correct predictions over the total predictions. A correct prediction is where the prediction labels equals the true (actual) labels. </p>"},{"location":"models/model/validation/metrics/#iou","title":"IoU","text":"<p>This metric is the intersection over union. The intersection is based on the logical AND operation of the true (actual) labels and the prediction labels. The union is based on the logical OR operation of the true (actual) labels and the prediction labels. </p>"},{"location":"models/model/validation/metrics/#model-timings","title":"Model Timings","text":"<p>The model inference timings are represented as a histogram where the y-axis shows the frequency of occurences in each bin in the x-axis. We can expect the speed of the model to be based on the bin with the highest frequency. However, the timings could be distributed across the bins to show variations in the model inference times.</p> Model Timings"},{"location":"models/model/validation/metrics/#confusion-matrix","title":"Confusion Matrix","text":"<p>The Confusion Matrix provides a summary of the prediction results by comparing the predicted labels with the ground truth (actual) labels. This matrix will show the ground truth labels along the x-axis and the predicted labels along the y-axis. Along the diagonal where both ground truth labels and prediction labels match shows the true positive (correct predictions) counts of that class. However, throughout validation, the matrix shows the cases where the model can misidentify labels or fail to find the labels.</p> Confusion Matrix"},{"location":"models/model/validation/metrics/#precision-vs-recall","title":"Precision vs. Recall","text":"<p>The precision vs. recall curve shows the tradeoff between precision and recall. At lower thresholds, precision will tend to be lower due to increased leniency for valid detections. However, more detections will tend to result in higher recall as the model finds more ground truth labels. Increasing the threshold will start to increase precision for more precise detections, but will start to reduce recall due to the reduction of model detections. The following curve shows the precision vs. recall trend for each of the classes in the dataset, along with the average curve for all the classes. A higher area under the curve, the better the model performance as this indicates maximized values for precision and recall throughout the varying thresholds. </p> Precision vs. Recall"},{"location":"models/model/validation/metrics/#fusion","title":"Fusion","text":"<p>This section will describe the validation metrics reported in Fusion validation sessions.</p>"},{"location":"models/model/validation/metrics/#base-metrics","title":"Base Metrics","text":"<p>The Fusion validation sessions reports the metrics for precision, recall, F1-score, and IoU represented as a bar chart. Shown below is an example. </p> Base Metrics <p>By default, these metrics are calculated based on the kernel sizes 1x1 and 3x3 which can be configured when starting a new session (See step 4. in Radar Validation). The kernel size is the window size setting where a kernel size of 1x1 indicates a 1-to-1 match between the ground truth and the model occupancy grid. A prediction can only be correct in a 1x1 kernel if the position of the prediction is in the same position as the ground truth. However, increasing the kernel size is more lenient by allowing predictions to be correct if their positions are within 3 meters away (3x3 kernel) from the ground truth. </p> <p>The metrics and their equations are described below.</p>"},{"location":"models/model/validation/metrics/#precision_1","title":"Precision","text":"<p>This metric is based on how well the model makes correct predictions. In other words, out of the total predictions, how many of these predictions were correct. The equation for precision is shown in the Glossary.</p>"},{"location":"models/model/validation/metrics/#recall_1","title":"Recall","text":"<p>This metric is based on how well the model finds the ground truth. In other words, out of the total ground truth, how many were found by the model. The equation for recall shown in the Glossary.</p>"},{"location":"models/model/validation/metrics/#f1-score","title":"F1-Score","text":"<p>This metric is based on both precision and recall. It measures how well the model performs overall in terms of how well the model makes correct predictions and finds the ground truth. The following table demonstrates the nature of the F1-score as a function of precision and recall.</p> F1-Score <p>The table highlights how F1 is the average between precision and recall over the diagonal where both precision and recall are equal. Furthermore, it also highlights how the F1-score needs both precision and recall to have very good scores in order to have a very good F1-score. As an example, consider a recall of 0.90, but a precision of 0.10, the final value of the F1-score is 0.18 which is quite poor. The same is true if the roles were switched where precision is 0.90, but recall is 0.10. The nature of the F1-score indicates that a well performing model requires both precision and recall to be high. </p> <p>It is also important to note that for certain use-cases precision is more important over recall and vice versa. For example, an application in the farming industry for identifying good crops vs. bad crops, one would argue that precision is more important than recall. It would be better to miss a bad crop than to identify a good crop as a bad crop. Another example for an application in safety that requires detection of people, one would argue that recall is more important than precision. It is better to misidentify an object for being a person than to miss an actual person in the scene.  </p> <p>The equation for the F1-score is shown below.</p> \\[ \\text{F1-score} = \\frac{2 * precision * recall}{precision + recall} \\]"},{"location":"models/model/validation/metrics/#iou_1","title":"IoU","text":"<p>This metric is defined as the intersection over union. It also measures how well the model performs overall by comparing the amount of correct predictions over the total amount of ground truths and the model predictions. This metric, however, is not as lenient as the F1-score as demonstrated by the following table.</p> IoU Score <p>Along the diagonal, it is easier to see why the IoU metric is not as lenient as the F1-score. It shows how for an equal value of precision and recall, the final IoU score is lower than the average. For example, a precision of 0.10 and a recall of 0.10, the IoU is 0.05 which is half of the average value. The scores are always lower than the average between the precision and recall, but as both precision and recall increases, the final IoU score approaches the average, but still does not reach it. Furthermore, the IoU scores also requires both precision and recall to have very high scores in order for the IoU to have a very high score. For example, if recall is 0.90 and precision is 0.10, the final IoU score is 0.10. The same is true if precision is 0.90 and recall is 0.10. This shows that a well performing model have very good scores for both precision and recall. </p> <p>The equation for the IoU score is shown below.</p> \\[ \\text{IoU} = \\frac{\\text{intersection}}{\\text{union}} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives} + \\text{false negatives}} \\]"},{"location":"models/model/validation/metrics/#model-timings_1","title":"Model Timings","text":"<p>These timings are measured in the same way as Modelpack as described under Model Timings in the Modelpack section.</p>"},{"location":"models/model/validation/metrics/#precision-vs-recall_1","title":"Precision vs. Recall","text":"<p>The precision vs. recall curve is based on varying detection thresholds from 0 to 1 in 0.05 steps. The principle in practice is that for lower thresholds precision is low, but recall is high and as the threshold increases, precision increases and recall decreases. This shows the tradeoff between precision and recall. The nature of this tradeoff is due to increased detections at low threshold thus capturing more ground truths (high recall) but much more prone to false predictions (low precision). The opposite is true for high thresholds. A well performing model shows a high area under the curve of the precision vs. recall curve. </p> Precision vs. Recall <p>Another representation of the precision vs. recall is to incorporate the varying threshold in the plot. The following curve shows the precision and recall vs. thresholds curve. As mentioned, at lower thresholds precision is low and recall is high, but increasing the threshold we can see precision and recall converge to a point. The point of convergence indicates the ideal threshold to use for deploying the model. This is the optimum threshold where precision and recall are balanced such that one is not sacrificing the other. </p> Precision and Recall vs Thresholds"},{"location":"models/model/validation/metrics/#bev-heatmaps","title":"BEV Heatmaps","text":"<p>There are four BEV heatmaps generated. The heatmaps are a representation of the occupancy grid that is the output of the Radar model. This occupancy grid is the field of view of the model that represents positions in the scene in meters. The BEV heatmaps provides indications where the model is generally making right or wrong predictions. Furthermore, the heatmaps also indicate how the ground truth is distributed across the dataset. </p> <p>Note: On a cell by cell basis, the sum of true positive, false positive, and false negative rates equals 1.</p>"},{"location":"models/model/validation/metrics/#true-positives-heatmap","title":"True Positives Heatmap","text":"True Positive Heatmap <p>The measurement is based on each cell. For each cell, what % of the sum of true positives, false positives, and false negatives were true positives. The equation for this heatmap is the following.</p> \\[ \\text{cell outcome} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives} + \\text{false negatives}} \\]"},{"location":"models/model/validation/metrics/#false-negatives-heatmap","title":"False Negatives Heatmap","text":"False Negative Heatmap <p>The measurement is based on each cell. For each cell, what % of the sum of true positives, false positives, and false negatives were false negatives. The equation for this heatmap is the following.</p> \\[ \\text{cell outcome} = \\frac{\\text{false negatives}}{\\text{true positives} + \\text{false positives} + \\text{false negatives}} \\]"},{"location":"models/model/validation/metrics/#false-positives-heatmap","title":"False Positives Heatmap","text":"False Positive Heatmap <p>This measurement is based on each cell. For each cell, what % of the sum of true positives, false positives, and false negatives were false positives. The equation for this heatmap is the following.</p> \\[ \\text{cell outcome} = \\frac{\\text{false positives}}{\\text{true positives} + \\text{false positives} + \\text{false negatives}} \\]"},{"location":"models/model/validation/metrics/#ground-truth-heatmap","title":"Ground Truth Heatmap","text":"Ground Truth Heatmap <p>This measurement is purely based on the ground truth counts. This heatmap provides indications of the concentration of samples in the dataset. This heatmap has no equation, it is the collection of ground truth counts throughout the experiment.</p>"},{"location":"models/model/validation/metrics/#glossary","title":"Glossary","text":"<p>This section will explain the definitions of key terms frequently mentioned throughout this page.</p> Term Definition True Positive Correct model predictions. The model prediction label matches the ground truth label. For object detection, the IoU and confidence scores must meet the threshold requirements. False Positive Incorrect model predictions. The model prediction label does not match the ground truth label. False Negative The absence of model predictions. For cases where the ground truth is a positive class, but the model prediction is a negative class (background). Precision Proportion of correct predictions over total predictions. \\(\\text{precision} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}\\) Recall Proportion of correct predictions over total ground truth. \\(\\text{recall} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}\\) Accuracy Propertion of correct predictions over the union of total predictions and ground truth. \\(\\text{accuracy} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives} + \\text{false positives}}\\) IoU The intersection over union. \\(\\text{IoU} = \\frac{\\text{intersection}}{\\text{union}} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives} + \\text{false negatives}}\\)"},{"location":"models/model/validation/modelpack/","title":"Modelpack","text":"<p>This page will provide a walk-through on using DVE for validating the performance of Vision models that have been trained using Modelpack, through the QuickStart Guide or Training Modelpack. This page will focus only on the validation of Modelpack.</p> <ol> <li> <p>Select Validator from the tool options.</p> <p> Tool Options </p> </li> <li> <p>Specify the project to run validation at the center of the top menu bar.</p> <p> Project Selection </p> </li> <li> <p>Create a new validation session by clicking the create button on the top right of the page.</p> <p> Create New Session </p> </li> <li> <p>Configure the settings on the left panel by specifying the name of the validation session, the model file to validate, and the dataset to deploy. Next configure the settings on the right panel by specifying the validation parameters.</p> <p>Note: Additional information on these parameters are provided by hovering over the info button. </p> <p> Validation Options </p> </li> <li> <p>Start the session by clicking the START SESSION button on the bottom right.</p> <p> Start the Session </p> </li> <li> <p>The validation session has now started while the progress is tracked on the left panel and additional information and status is shown on the right panel. </p> <p> Validation Session </p> </li> <li> <p>Once completed, the status will be shown as complete.</p> <p> Completed Session </p> </li> <li> <p>The metrics are shown by clicking the plots button on the top left of the session card. </p> <p> Validation Metrics </p> <p>Note: See Metrics for further details.</p> </li> <li> <p>It is also possible to compare validation metrics for multiple sessions. This is done by checking the checkboxes on the top left of the session cards.</p> <p> Comparing Sessions </p> <p>Compare the validation sessions by clicking the COMPARE VALIDATE SESSION button on the top left. This will display the validation metrics side by side for the specified validation sessions.</p> <p> Metrics Side-by-Side </p> </li> </ol>"},{"location":"modules/","title":"EdgeFirst Modules User Manual","text":"<p>This is the EdgeFirst Modules User Manual.  It provides information on how to use the EdgeFirst Perception Middleware in both the vision-only Maivin configuration and the combined vision and radar Raivin configuration.  If you have purchased a Maivin or Raivin vision module, please continue on to the QuickStart Guide for an unboxing of your new module.</p>"},{"location":"modules/advanced_foxglove/","title":"Advanced Foxglove","text":"<p>This section will describe two, more advanced tasks with Foxglove Studio: 1. Using the 3D Panel to view the post-processed Radar outputs contained in the Fusion Targets topics. 2. Using the User Scripts panel to created a short Typescript script that also outputs the values for those topics.</p> <p>It is assumed you have read the introduction to Foxglove Studio, familiarized yourself with the application, and have downloaded an MCAP that has recorded the <code>/fusion/targets</code> topic.</p>"},{"location":"modules/advanced_foxglove/#the-fusion-targets-topic","title":"The Fusion Targets Topic","text":"<p>The Fusion Targets topic contains post-processed radar outputs that identify targets as people.  For each target it detects, it returns the following information about the target: * x, y, and z co-ordinates of the target, in metres * speed, in metres per second * power * radar cross section * cluster ID * Fusion Class * Vision Class</p> <p>The first six fields are generated from the Radar, while the last three are generated by the Fusion service.  Each of the nine fields is 4-bytes in size and is noted in the <code>/fusion/targets</code> topic <code>fields</code> field. </p> <p>Each target takes up 36 bytes of data which is noted in the \"point_step\" field.  There is a \"width\" field in the topic which counts the number of targets, a \"row_step\" field that counts the number of bytes of raw data in the topic, such that <code>row_step = point_step * width</code>.  We can see these fields in a Raw Messages Panel looking at the <code>/fusion/targets</code> topic; the nine items in the \"fields\" field being the per-target information described above. </p> <p>The Fusion and Vision Class are produced by Fusion service on the Raivin.  They will report 0 for any cluster not identified as a person and 1 for any cluster as belonging to a human.  The Raivin WebUI can use either class or both classes together to determine if any radar target is a person or not.</p>"},{"location":"modules/advanced_foxglove/#3d-panes-for-fusion","title":"3D Panes for Fusion","text":"<p>To Create a 3D view for the Fusion Class: 1. From a panel in Foxglove, click the \"More\" kebob icon and change the panel to a \"3D\" panel.  2. Click the \"Settings\" gear icon for the panel to open up the Settings Panel.  3. Change the title from 3D to \"Vision Class\" 4. In the \"Topics\" side-panel, click the \"visibility\" closed-eye icon on the \"/fusion/targets\" to view the topic.  5. Expand the \"/fusion/topics\" settings and configure the following settings:    1. Point Size to \"10\"    2. Color mode to \"Gradient\"    3. Color by to \"vision_class\"    4. Set Gradient values to \"000000ff\" (black) and \"00ff15ff\" (green)    5. Set Value min to \"0\" and \"max to \"1\"  6. Expand the \"Custom Layer\" side-panel, and then the \"Grid\" settings and configure the following settings.    1. Set Size and Divisions to \"12\"    2. Set the \"Position X\" setting to \"6\" </p> <p>Once all that is finished, you should have a panel that looks as the left panel as follows - camera panel also shown for comparison. </p>"},{"location":"modules/advanced_foxglove/#adding-vision-class","title":"Adding Vision Class","text":"<p>The steps above and be followed for Vision Class as well, but please note - you cannot have both classes on the same 3D panel, as each topic is only allowed once.  If you want, you can also do these in an easier fashion. 1. On the \"More\" kebob icon of the fusion class, split the panel (in this case, down)    2. In the bottom Fusion Class panel, make the following changes:    1. Change the \"Title\" to Vision Class    2. In the \"/fusion/targets\" settings, change \"Color by\" to \"vision_class\" and the right gradient to \"ff8300ff\" (orange)   </p> <p>The new panels should looks as below. </p> <p>In the bottom right corner of this screenshot, we see a \"/studio_script/output_topic\" Raw Messages Panel.  This is the output of TypeScript user script created in FoxGlove that we will describe in the next subection.</p>"},{"location":"modules/advanced_foxglove/#user-scripting","title":"User Scripting","text":"<p>FoxGlove has a \"User Scripts\" panel that uses TypeScript to manipulate the MCAP topics.  This section will show a script created extract the \"fusion_class\" and \"vision_class\" values of each target and output if either class is reporting a person from the radar output.</p>"},{"location":"modules/advanced_foxglove/#the-user-script-interface","title":"The User Script Interface","text":"<p>When you change a panel to a User Scripts panel, you will see the Welcome splash screen.  The important icon in the left sidebar is the top one, the \"Scripts\" scroll icon, which will open any scripts you have created for editing.  The \"Ulitilies\" toolbox icon, which displays the various importable libraries, is also important and worth reviewing.  If we have script already created, we can click on it to edit it.  It is recommended that you open the \"User Scripts\" panel in fullscreen if you create scripts in it. </p> <p>There is one script, \"fusion_decoder\", which is the script we can look at. Since you do not have this script, it is included below - you can copy-and-paste this into a new script panel.</p>"},{"location":"modules/advanced_foxglove/#the-script","title":"The Script","text":"<p>Here is the \"fusion_decoder\" script.</p> <p><pre><code>import { Input } from \"./types\";\nimport { Float32Reader } from \"./readers\";\n\nexport const inputs = [\"/fusion/targets\"];\nexport const output = \"/studio_script/output_topic\";\n\nexport default function script(event: Input&lt;\"/fusion/targets\"&gt;): {\n  Person: Array&lt;string&gt;;\n} {\n  let targets = event.message.width;\n  let d: Uint8Array = new Uint8Array(event.message.data);\n  let p_step = event.message.point_step;\n  let out_array: Array&lt;string&gt; = [];\n\n  for (let ii = 0; ii &lt; targets; ii++) {\n    let f32r = new Float32Reader(ii * p_step);\n    let fusion = f32r.read(d, 28);\n    let vision = f32r.read(d, 32);\n\n    if (fusion + vision &gt; 0) {\n      let temp = `Fusion: ${fusion} Vision: ${vision}`;\n      out_array.push(temp);\n    }\n  }\n  return { Person: out_array };\n}\n</code></pre> We first need to import the \"Input\" class from the \"./types\" library as it is the class type of input topics.  We also need a reader to parse the 32-bit Floating Point numbers in from the 8-bit unsigned integer data array.  Lastly, we need to declare the input and output topics we will be using in the script.</p> <p>This default function will take an inputed topic and return an array of strings with the name of \"Person\".  Referring back to the image in the previous section, we see that this is a Raw Messages Panel for the output topic, <code>/studio_script/output_topic</code> which comes from the script <code>fusion_decoder</code>, and is a list of strings named \"Person\".</p> <p>As <code>event</code> is the variable name for the <code>/fusion/targets</code> topic, we can use the <code>message</code> instance variable to access the topic elements.  For example, <code>event.message.width</code> and <code>event.message.point_step</code> refer to the \"width\" and \"point steps\" fields discussed above.  We set the topic data to a Uint8Array named <code>d</code> for short-hand and initialize the output array.</p> <p>We then have the for loop that goes through each Fusion target and determines what the Fusion and Vision classes report.  We create a new reader at the starting offset of the target <code>ii*p_step</code>, and read the values at offsets 28 and 32, respectively for <code>fusion_class</code> and <code>vision_class</code>.  If either value is 1, it will append the infomration to the output array.</p> <p>Once all the targets have been processed, the function returns the array of strings as the \"Person\" array.  To confirm this is done correctly, we should check to see if the new <code>/studio_script/output_topic</code> topic exists.  If you have a \"Data Source Info\" panel up (or create one), you should see the topic in the list:  If it exists, we should be able to load the topic in a Raw Messages Panel as we did above.</p> <p>Using this script as a base to expand functionality should be quite easy.  For example, let's not just output the Fusion and Vision class values for each target person; let's also include Cluster ID and X,Y,Z distances.  We can do this by replacing the inside of the if statement with the following code.</p> <pre><code>    if (fusion + vision &gt; 0) {\n      let x = Math.round(f32r.read(d, 0) * 100) / 100;\n      let y = Math.round(f32r.read(d, 4) * 100) / 100;\n      let z = Math.round(f32r.read(d, 8) * 100) / 100;\n      let id = f32r.read(d, 24);\n\n      out_array.push(`tar: ${id} (${x}, ${y}, ${z}) F: ${fusion} V: ${vision}`);\n    }\n</code></pre> <p></p> <p>For more detailed information about Foxglove Studio features, visit the Foxglove Documentation website.</p>"},{"location":"modules/architecture/","title":"System Architecture","text":"<p>The EdgeFirst Perception Modules run a Linux operating system based on Toradex's Torizon, we refer to the operating system as Torizon for Maivin.  The operating system packages include common libraries and tools you would expect to find on a typical embedded Linux operating system focused on computer vision and AI (Python, OpenCV, etc...).  </p> <p>Central to the EdgeFirst Perception Modules is the EdgeFirst Perception Middleware, what we call our collection of applications and libraries used in the implementation of the perception stack.  The details of the low-level libraries are covered in the EdgeFirst Perception Developer Guide, for  this document we focus on describing the application services and how they fit together to deliver  the perception middleware.</p>"},{"location":"modules/architecture/#middleware-services","title":"Middleware Services","text":"<p>The Perception Middleware is modular and split into various application services, each focused on a general task.  For example a camera service is charged with interfacing with the camera and ISP (Image Signal Processor) to efficiently deliver camera frames to other services who need access to the camera.  The camera service is also responsible for encoding camera frames using a video codec into H.265 video for efficient recording or remote streaming, this feature of the camera service can be configured or disabled if recording or streaming are not required.</p> <p>The middleware services communicate with each other using the Zenoh networking middleware which provides a highly efficient publisher/subscriber communications stack.  This architecture is similar to ROS2 and our services encode their messages using the ROS2 CDR (Common Data Representation). We use the ROS2 standard schemas where applicable and augment with our own custom schemas where required.  The Recorder and Foxglove chapters go into more detail on how this allows efficient streaming and recording of messages and interoperability with industry standard tools.</p> <pre><code>graph LR\n    camera --&gt; model[\"vision model\"] --&gt; zenoh    \n    radarpub --&gt; fusion[\"fusion model\"] --&gt; zenoh\n    camera --&gt; fusion\n    radarpub --&gt; zenoh\n    camera --&gt; zenoh    \n    model --&gt; fusion\n    navsat --&gt; zenoh\n    imu --&gt; zenoh\n    zenoh --&gt; recorder --&gt; mcap\n    zenoh --&gt; webui --&gt; https\n    zenoh --&gt; user[\"user apps\"]\n    https --&gt; user</code></pre>"},{"location":"modules/configuration/","title":"Configuration","text":"<p>This section describes the various settings pages and what they do.  The root Settings Page can be reached by clicking the rightmost gear icon on the top ribbon of any page of the Raivin web-interface. </p> <p>Every settings page has a \"Save Configuration\" button at the bottom of the page. If you make changes, click this button to save them. </p>"},{"location":"modules/configuration/#mcap-recorder-settings-page","title":"MCAP Recorder Settings Page","text":"<p>This page configures how sensor and processed outputs are saved on the MCAP Recording Page. </p> <pre><code>These values are stored in the `/etc/default/recorder` file on the device and can be hand-edited.  This is not recommended.\n</code></pre>"},{"location":"modules/configuration/#storage-location","title":"Storage Location","text":"<p>This controls the storage location for the MCAP recordings.  If using an SD Card, this should point to /media/DATA or adjusted for the SD Card mount point.</p>"},{"location":"modules/configuration/#compression","title":"Compression","text":"<p>This controls the MCAP compression level.  Compression will impact the CPU usage compared to no compression but can have significant benefits to MCAP size.  It is most impactful when recording the <code>/radar/cube</code> or <code>/model/mask</code> topics.  Options are <code>none</code>, <code>lz4</code>, and <code>zstd</code>.  The LZ4 compression is faster while ZSTD provides better compression.  No compression is the fastest but creates files up to 3x larger.</p>"},{"location":"modules/configuration/#topics-recording","title":"Topics recording","text":"<p>The rest of the page reports upon which topics are recording and allows the user to enable or disable certain optional topics.  Mandatory topics that cannot be changed on the page include: - /tf_static: This is a meta-topic that includes information from all recorded topics. - /gps: This is the GPS topic that includes latitude, longitude, elevation, etc. - /imu: This is where the IMU sensor exports its information, and includes acceleration and orientation information about the vision module. - /camera/info: This includes information about the video sensor. - /camera/h264: This contains the raw-video output of the video sensor, in H.264 format.</p> <p>These topics are parts of the \"Localization Topics\" and \"Camera Topics\", which are currently locked.</p> <p>Clicking on the \"Model Topics\" box will enable recording of all the Model and Fusion recording topics, as described below.</p>"},{"location":"modules/configuration/#model-recording","title":"Model Recording","text":"<p>Checking the \"/model\" topic box enables the following topics: - /model/info: This contains information regarding the model being run. - /model/boxes2d: This includes detection boxes from the model, if the model supports object detection. - /model/mask_compressed: This contains the output of the segmentation model if running.</p>"},{"location":"modules/configuration/#fusion-recording","title":"Fusion Recording","text":"<p>Checking the \"/fusion\" topic box enables recording of the following topics: - /fusion/targets: This contains information regarding the analysis of the radar targets, specifically which are people and which are not. - /fusion/occupancy: This contains information regarding the occupancy of targets on a polar grid emanating from the camera.</p>"},{"location":"modules/configuration/#radar-recording","title":"Radar Recording","text":"<p>Checking the \"/radar\" topic box enables the following topics: - /radar/info: This contains information regarding the radar. - /radar/targets: This contains the target information from the radar sensor. - /radar/clusters:  This contains the clustered information from the radar sensor. It also opens up the \"/radar/cube\" topic that can be enabled.</p>"},{"location":"modules/configuration/#radar-cube","title":"Radar Cube","text":"<ul> <li>/radar/cube: The raw radar cube data output.  This can be a lot of data, and we provide expose a setting to user to limit this.</li> </ul> <p>The Radar Cube FPS setting limits the radar cube framerate to reduce recording size.  While the camera topic is encoded with H.264 which makes use of keyframes to significantly reduce the topic size, no such compression is available for the radar cube which means only the COMPRESSION parameter applies (if enabled).  While capturing datasets, the full 18 FPS is typically not required and it is recommended to set this parameter to 1-5 FPS to reduce the MCAP size.</p>"},{"location":"modules/configuration/#camera-settings","title":"Camera Settings","text":"<p>This page configures the camera service that interacts with the Raivin's OmniVision OS08A20 image sensor.  With the exception of the H.264 Bitrate and maybe camera or stream sizes, it is not recommended that you change these settings. </p> <pre><code>These values are stored in the `/etc/default/camera` file on the device and can be hand-edited.  This is not recommended.\n</code></pre>"},{"location":"modules/configuration/#camera-device","title":"Camera Device","text":"<p>This configures what camera device the camera service will use, which on Raivin will be <code>/dev/video3</code>.</p>"},{"location":"modules/configuration/#camera-size","title":"Camera Size","text":"<p>This sets the camera resolution which is used for the camera capture and separate from the streaming resolution. The camera resolution can be set to any resolution supported by the camera.</p>"},{"location":"modules/configuration/#stream-size","title":"Stream Size","text":"<p>This configures the streaming resolution for the H.264 and JPEG streaming.  The H.264 stream supports up to HD resolution (1920x1080).  The JPEG stream supports all resolutions but is encoded on the CPU so the practical limit is around 960x540 or 640x360 to maintain 16:9 aspect ratio.</p>"},{"location":"modules/configuration/#mirror","title":"Mirror","text":"<p>The camera mirror setting can flip the camera image to match the orientation of the camera. The camera can also be mirrored horizontally to provide a mirror image.  Accepted values are \"none\", \"horizontal\", \"vertical\", and \"both\".  This is set to \"both\" as the image sensor is both upside-down and inverted.</p>"},{"location":"modules/configuration/#h264-bitrate","title":"H.264 Bitrate","text":"<p>Controls the H.264 streaming compression level.  The higher the bitrate, the better the quality of the image.  The actual bitrate remains variable based on the scene but this value sets the cap. Possible values are \"auto\", \"mbps5\", \"mbps25\", \"mbps50\", and \"mbps100\".  The \"auto\" setting is about 10 Mbps on the Raivin. This also impacts MCAP recording size.</p>"},{"location":"modules/configuration/#h264-streaiming","title":"H.264 Streaiming","text":"<p>This setting enables or disables the /camera/h.264 topic.</p>"},{"location":"modules/configuration/#jpeg-streaming","title":"JPEG Streaming.","text":"<p>This setting enables or disables the /camera/jpeg topic.</p>"},{"location":"modules/configuration/#webui-settings-page","title":"WebUI Settings Page","text":"<p>This page configures how information is displayed on the Segmentation Page. </p> <pre><code>These values are stored in the `/etc/default/webui` file on the device and can be hand-edited.  This is not recommended.\n</code></pre>"},{"location":"modules/configuration/#mirror-inputs","title":"Mirror Inputs","text":"<p>This mirrors both the Segmentation View as well as the Occupancy Grid on the Segmentation page.</p>"},{"location":"modules/configuration/#angle-bins","title":"Angle Bins","text":"<p>These three settings set the left-side minimum, right-side maximum, and width size (in degrees) of the angular radar views on the Segmentation and Occupancy pages.  The below image shows a minimum of -70, maximum of 70, and a binwidth of 14. </p>"},{"location":"modules/configuration/#range-bins","title":"Range Bins","text":"<p>These three settings set the near-side minimum, far-side maximum, and width size (in meters) of the radar views on the Segmentation and Occupancy pages.  The below image shows a minimum of 2, maximum of 9, and a binwidth of 1. </p>"},{"location":"modules/configuration/#draw-pcd","title":"Draw PCD","text":"<p>There are three settings for drawing the Point Cloud Data (PCD): - Grid Draw PCD: This controls drawing the PCD on the Occupancy Page - Combined Grid Draw PCD: This controls drawing the PCD on the Occupancy Grid on the Segmentation Page. - Combined Camera Draw PCD: This controls drawing the PCD on the Segementation View on the Segmentation Page.</p> <p>Each of these settings can be configured to disable drawing the PCD, or draw it based on radar power, radar cross-section (RCS) area, speed of object, or post-processed output from the Fusion model, Vision model, or Combined from both.</p>"},{"location":"modules/configuration/#topics","title":"Topics","text":"<p>The topic settings control the following: - Mask Topic: This sets what topic the Segmentation View uses as input to draw segmentation masks. - Detect Topic: This sets what topic the Segmentation View uses as input to draw detection boxes. - PCD Topic: This sets what topic the Segmentation View and both Occupancy Grids use as input to draw radar data. - H264 Topic: This sets what topic the Segmentation View uses to get the video feed.</p>"},{"location":"modules/configuration/#draw-boxes-and-draw-boxes-text","title":"Draw Boxes and Draw Boxes Text","text":"<p>The settings turn on objection detection boxes and text from the Detect Topic to display on the Segmentation View.  If there is no Detect Topic, boxes will not be displayed.</p>"},{"location":"modules/configuration/#show-stats-and-people-count","title":"Show Stats and People Count","text":"<p>These settings enable statistics views and a people counter on the Segmentation and Occupancy Pages.  The statistics are near the top right of the screen while the people counter is at the bottom right. </p>"},{"location":"modules/configuration/#model-configuration","title":"Model Configuration","text":"<p>These settings configure the perception engine that is processing input from the video sensor and providing output on the model topics.  <pre><code>These values are stored in the `/etc/default/model` file on the device and can be hand-edited.  This is not recommended.\n</code></pre></p>"},{"location":"modules/configuration/#model","title":"Model","text":"<p>A model is required for the model application. This can be a segmentation model and/or a detection model.</p>"},{"location":"modules/configuration/#engine","title":"Engine","text":"<p>The model can be run using different computation engines.  The default engine is the i.MX 8M Plus NPU.  Other options are the CPU or the GPU.</p>"},{"location":"modules/configuration/#detection-only-settings","title":"Detection-only Settings","text":"<p>The following settings will only impact detection-based output in the <code>/model/boxes2d</code> topic with models that output object detection results.</p>"},{"location":"modules/configuration/#visualization","title":"Visualization","text":"<p>Enables publishing the legacy visualization message for detection models.  It is 'false' by default which means only the primary model box and mask topics will be available.  If using Foxglove Studio make sure you install the EdgeFirst for Foxglove Plug-in.</p>"},{"location":"modules/configuration/#threshold","title":"Threshold","text":"<p>Score threshold sets the minimum detection score before a bounding box is generated for the inferred object for detection.</p>"},{"location":"modules/configuration/#iou","title":"IOU","text":"<p>Detection IoU controls the minimum overlap for merging boxes during NMS.  A larger number will produce more boxes with some overlap while a smaller number will generate fewer boxes.</p>"},{"location":"modules/configuration/#max_boxes","title":"MAX_BOXES","text":"<p>The maximum number of detection boxes which can be generated.</p>"},{"location":"modules/configuration/#label_offset","title":"LABEL_OFFSET","text":"<p>The label offset is required for certain models to account for differences in background class handling relative to the labels.  It should usually be zero but some odd configurations will sometimes require 1 or -1 for offset.</p>"},{"location":"modules/configuration/#track-settings","title":"Track Settings","text":"<p>These settings impact object tracking with object detection.  They have no effect for segmentation-only models.</p>"},{"location":"modules/configuration/#track","title":"TRACK","text":"<p>This turns on the ByteTrack tracker. This is useful for smoothing bounding boxes across frames, and for associating multiple detections over time to a single object. None of the other settings will have an effect if this is set to 'false'.</p>"},{"location":"modules/configuration/#track-extra-lifespan-seconds","title":"Track Extra Lifespan (seconds)","text":"<p>The number of seconds a tracked object can be missing for before being removed from tracking.</p>"},{"location":"modules/configuration/#track_high_conf","title":"TRACK_HIGH_CONF","text":"<p>The high confidence threshold for the ByteTrack algorithm.</p>"},{"location":"modules/configuration/#track_iou","title":"TRACK_IOU","text":"<p>The tracking IoU threshold for box association. Higher values will require boxes to have higher IoU to the predicted track location to be associated.</p>"},{"location":"modules/configuration/#track_update","title":"TRACK_UPDATE","text":"<p>The tracking update factor. Higher update factor will mean less smoothing but more rapid response to change. Use values from 0.0 to 1.0. Values outside this range will cause unexpected behaviour.</p>"},{"location":"modules/configuration/#segmentation-only-settings","title":"Segmentation-only Settings","text":"<p>The following setting will only impact segmentation-based output in the <code>/model/mask_compressed</code> or <code>/model/mask</code> topisc with models that output segmentation results.</p>"},{"location":"modules/configuration/#mask_compression","title":"MASK_COMPRESSION","text":"<p>Enable compression for segmentation masks.  When enabled, both the <code>/model/mask</code> and <code>/model/mask_compressed</code> topics will be available.  The compressed mask should be used from remote connections while the un-compressed topic should be used from local connections to avoid redundant compress/decompress steps. <pre><code>Turning off mask compression will disable the `/model/mask_compressed` topic.  The WebUI will need to have its [Mask Topic](./configuration.md#topics) changed to `/model/mask`.  The segmentation mask will also not be recorded.\n</code></pre></p>"},{"location":"modules/configuration/#openvx-graph-caching","title":"OpenVX Graph Caching","text":"<p>The OpenVX driver provides a graph caching mechanism that significantly speeds up future reload times for the graph.  The cache stores a binary representation of the in-memory graph, not the model used to generate the graph, to the file system.  When loading a model, graph generation proceeds as normal but the time-consuming graph loading step will first attempt to load the cached representation.  </p> <p>The driver allows for many graphs to be stored and the location, along with enabling the feature, is controlled through a pair of environment variables.  This allows a user to select on a per-process basis when to enable graph cache and where to store the cache.</p> <p>These settings enable and configure this functionality</p>"},{"location":"modules/configuration/#viv_vx_enable_cache_graph_binary","title":"VIV_VX_ENABLE_CACHE_GRAPH_BINARY","text":"<p>This option will enable the graph caching which significantly speeds up load times for models.  If you encounter issues set to 0 to disable.</p>"},{"location":"modules/configuration/#viv_vx_cache_binary_graph_dir","title":"VIV_VX_CACHE_BINARY_GRAPH_DIR","text":"<p>This control the NPU graph cache storage location.</p>"},{"location":"modules/configuration/#service-status","title":"Service Status","text":"<p>This page allows users to enable and disable the nine main services used by the Raivin.  By default, each of these services but the Recorder service should be running and enabled. </p>"},{"location":"modules/configuration/#radar-configuration-raivin-only","title":"Radar Configuration (Raivin-only)","text":"<p>This page configures the radar publishing service that interacts with the Raivin's integrated DRVEGRD-169 radar module from smartmicro.  With the exception of the H.264 Bitrate and maybe camera or stream sizes, it is not recommended that you change these settings. </p> <pre><code>These values are stored in the `/etc/default/radarpub` file on the device and can be hand-edited.  This is not recommended.\n</code></pre>"},{"location":"modules/configuration/#log-level","title":"Log Level","text":"<p>Log level for the application, relevant sub-filters are radarpub and drvegrd refer to RUST_LOG documentation for details.</p>"},{"location":"modules/configuration/#center-frequency","title":"Center Frequency","text":"<p>Radar center frequency band.  The low option is required when using the ultra-short FREQUENCY_SWEEP option. Options are 'low', 'medium', and 'high'.</p>"},{"location":"modules/configuration/#frequency-sweep","title":"Frequency Sweep","text":"<p>The frequency sweep controls the detection range of the radar.  The following breakdown gives a general range at which a vehicle-type object should be detected.</p> <ul> <li>ultra-short: 9m (requires CENTER_FREQUENCY=\"low\")</li> <li>short: 19m</li> <li>medium: 56m</li> <li>long: 130m <pre><code>Ultra-short range (in Frequency Sweep) only works with Low Center frequency.\n</code></pre></li> </ul>"},{"location":"modules/configuration/#range-toggle","title":"Range Toggle","text":"<p>The range-toggle mode allows the radar to alternate between various frequency sweep configurations.  Applications must handle range toggling as targets will not be consistent between messages as the frequency alternates.</p> <p><pre><code>The radar cube model does NOT currently handle alternating frequencies as it requires the dataset to be captured at a specific frequency sweep config.\n</code></pre> Options are 'off', 'short-medium', 'short-long', 'medium-long', 'long-ultra-short', 'medium-ultra-short', and 'short-ultra-short'.</p>"},{"location":"modules/configuration/#detection-sensitivity","title":"Detection Sensitivity","text":"<p>The detection sensitivity only affects the radar target list (point-cloud) and controls the sensitivity to recognize a target.  The default is 'medium', the 'low' and 'high' sensitivity options will result in less and more targets, respectively.</p>"},{"location":"modules/configuration/#enable-cube","title":"Enable Cube","text":"<p>Enable streaming the low-level radar cube.  This can be used by the low-level # radar fusion model or to record MCAP files for the purpose of training this model.  The radar cube capture consumes approximately 240mbps bandwidth over the private Raivin ethernet connection and approximately 75% of one of the CPU cores, enable it when required but otherwise should be left disabled.</p>"},{"location":"modules/configuration/#clustering-options","title":"Clustering Options","text":"<p>The following settings enables and configures clustering the radar targets.</p>"},{"location":"modules/configuration/#clustering","title":"Clustering","text":"<p>Enable clustering the radar targets into the /radar/clusters topic.</p>"},{"location":"modules/configuration/#window-size","title":"Window Size","text":"<p>Temporal clustering of the radar targets using the window size.  The window size is the number of frames to cluster, each frame representing 55ms.  The window clustering is a rolling window so it does not incur any additional latency.</p>"},{"location":"modules/configuration/#clustering-eps","title":"Clustering EPS","text":"<p>The eplison value to be used for Density-Based Spatial Clustering and Apllication with Noise(DBSCAN) clustering. Higher values mean points further from each other can be clustered together.</p>"},{"location":"modules/configuration/#clustering-parameter-scale","title":"Clustering Parameter Scale","text":"<p>Clustering DBSCAN parameter scaling. Parameter order is x, y, z, speed. Set the appropriate axis to 0 to ignore that axis. Default setting of <code>[0.5 1.0 0.0 0.0]</code> means that only xy distances are taken into account.</p>"},{"location":"modules/configuration/#clustering-point-limit","title":"Clustering Point Limit","text":"<p>The minimum number of points per cluster for DBSCAN clustering.</p>"},{"location":"modules/configuration/#fusion-configuration-raivin-only","title":"Fusion Configuration (Raivin-only)","text":"<p>These settings configure the perception engine that is processing input from the radar sensor and providing output on the Fusion topics.  <pre><code>These values are stored in the `/etc/default/fusion` file on the device and can be hand-edited.  This is not recommended.\n</code></pre></p>"},{"location":"modules/configuration/#log-level_1","title":"Log Level","text":"<p>Log level for the application, relevant sub-filters include 'maivin-fusion'.  Refer to RUST_LOG documentation for details.</p>"},{"location":"modules/configuration/#radar-input-topic","title":"Radar Input Topic","text":"<p>The radar topic to use for a source of targets.  This is typically '/radar/clusters' which provides better target stability compared to the raw '/radar/targets' topic.</p>"},{"location":"modules/configuration/#occlusion-angle-limit-and-occlusion-range-limit","title":"Occlusion Angle Limit and Occlusion Range Limit","text":"<p>Points that are more than the \"Occlusion Range Limit\", in meters, behind and less than the  \"Occlusion Angle Limit\", in degrees, from a classified point will not be classified. This prevent false positives from points that should be occluded.</p>"},{"location":"modules/configuration/#occupancy-settings","title":"Occupancy Settings","text":"<p>The following settings are only used when the radar PCD is unclustered, usually by setting the radar input topic to <code>/radar/targets</code> instead of <code>/radar/clusters</code>.  These settings configure the way Fusion service generates occupancy output from the unclustered radar PCD.</p>"},{"location":"modules/configuration/#threshold_1","title":"Threshold","text":"<p>The number of required targets to acknowledge the target as real as opposed to noise that is filtered out.</p>"},{"location":"modules/configuration/#range-bin-limit","title":"Range Bin Limit","text":"<p>The minimum and maximum range to use for range bins, in meters.</p>"},{"location":"modules/configuration/#range-bin-width","title":"Range Bin Width","text":"<p>The size of each range bin, in meters.</p>"},{"location":"modules/configuration/#angle-bin-limit","title":"Angle Bin Limit","text":"<p>The minimum and maximum angles, in degrees, to use for angle bins. 0 degrees is forward.</p>"},{"location":"modules/configuration/#angle-bin-width","title":"Angle Bin Width","text":"<p>The size of each angle bin, in degrees.</p>"},{"location":"modules/configuration/#bin-delay","title":"Bin Delay","text":"<p>Bin delay in radar message count.  Each cell needs to be valid for \"bin delay\" frames before it is drawn. The cell stops being drawn after being invalid for \"bin delay\" frames.</p>"},{"location":"modules/foxglove/","title":"Foxglove Studio","text":"<p>Foxglove Studio is an open source application developed by FoxGlove Technologies, Inc.  It is part of the Robot Operating System (ROS) ecosystem and supports playback for MCAP recordings.  You can download Foxglove Studio as well as our EdgeFirst plug-in for Foxglove and customized Raivin Foxglove layout.</p>"},{"location":"modules/foxglove/#getting-started","title":"Getting Started","text":"<p>Let's discuss how to install our custom plug-ins once you've installed Foxglove Stuiods.</p>"},{"location":"modules/foxglove/#installing-edgefirst-plugin","title":"Installing EdgeFirst Plugin","text":"<pre><code>These will describe installing the 1.0.2 version of the EdgeFirst plug-in for Foxglove, which is current as of time of writing.\n</code></pre> <ol> <li> <p>Open Foxglove Studio</p> </li> <li> <p>If necessary, uninstall any existing versions of the EdgeFirst plug-in</p> </li> <li>Click the User Settings button on the right side of the top menu bar.</li> <li>Select the Extensions option in the pull-down menu. </li> <li>Click the EdgeFirst Detect plugin </li> <li>Click the \"Uninstall\" button </li> <li> <p>Click the \"Back to dashboard\" button in the top-left corner of the application main window.</p> </li> <li> <p>Install the current version of the EdgeFirst plug-in.</p> </li> <li>Click the User Settings button on the right side of the top menu bar.</li> <li>Select the Extensions option in the pull-down menu. </li> <li>Click the \"Install local extension...\" button. </li> <li> <p>Select the <code>edgefirst.detect-1.0.2.foxe</code> file from the downloads directory.</p> </li> <li> <p>Confirm that the 1.0.2 version was installed </p> </li> <li> <p>Close Foxglove Studio and restart it.</p> </li> </ol>"},{"location":"modules/foxglove/#installing-foxglove-layout","title":"Installing Foxglove Layout","text":"<p>We have included a custom Raivin Layout for Foxglove Studio.To install it, please download it and follow the instructions below.</p> <ol> <li>Open Foxglove Studio</li> <li>Load an MCAP file downloaded from the Raivin.</li> <li>Click the \"Layout\" buttom in the top taskbar.</li> <li>Select the \"Import from file...\" option in the Layout menu. </li> <li>Go to the download directory holding the JSON layout file and select the file.</li> <li>Confirm the layout JSON file is loaded. </li> </ol>"},{"location":"modules/foxglove/#layout-features","title":"Layout Features","text":"<p>The default Raivin layout includes: - Top panel: H.264 camera stream with bounding box overlays   - Shows detection results (i.e. the coloured boxes around people when using a detection model)   - Shows segementation masks (i.e. the coloured blobs covering detected objects when using a segmentation model) - Bottom right panel: GPS coordinates map view   - Interactive map with zoomable blue target showing camera position - Bottom left panel: IMU sensor readings   - Can be configured to show real-time plots - Bottom timeline: Playback controls for navigation</p>"},{"location":"modules/foxglove/#visualization-features","title":"Visualization Features","text":"<p>In the example above, we see both detection boxes and segmentation masks in the H.264 Camera topic.</p>"},{"location":"modules/foxglove/#viewing-detection-messages","title":"Viewing Detection Messages","text":"<p>The detection boxes are contained in the <code>/model/boxes2d</code> topic.  By default, this topic is not enabled.  See the Maivin Dataset Recording section for details on how to enable this topic and record an MCAP with the Raivin.</p> <pre><code>Not all vision models are able produce detection results.  The default model on the Raivin can produce both detection and segmentation results.\n</code></pre> <ol> <li>Record a MCAP file that captures the <code>/model/boxes2d</code> topic.</li> <li>Confirm with the \"Details\" button that the newly recorded MCAP has a <code>/model/boxes2d</code> topic. </li> <li>Download the file from the Raivin and load it in Foxglove Studio.</li> <li>Click the \"Settings\" gear icon on the right side of the <code>/camera/h264/</code> panel task bar.</li> <li>The <code>/model/boxes2d</code> option should appear in the \"Image annotations\" dropdown menu in the \"Image Panel\" settings sidebar (bottom left of image below). </li> <li>Enable the <code>/model/boxes2d</code> image annotations by clicking the closed eye icon. This will draw boxes around the detected objects. </li> </ol>"},{"location":"modules/foxglove/#viewing-segmentation-messages","title":"Viewing Segmentation Messages","text":"<p>Segmentation masks are contained in the <code>/model/mask_compressed</code> topic which is enabled on the Raivin by default.  Because of the amount of data included within this stream, it is not compatible with the default Image drawing API in Foxglove Studio. To visualize the Segmentation Mask, the EdgeFirst Foxglove plug-in must be installed to view segmentation masks in Foxglove.</p> <p>The instructions to view these masks are the same as above but using the <code>/model/mask_compressed</code> topic instead of the <code>/model/boxes2d</code> topic. </p>"},{"location":"modules/foxglove/#viewing-radarcube-messages","title":"Viewing /radar/cube Messages","text":"<p>By default, none of the radar topics are recorded as part of an MCAP file.  The Image panel in Foxglove can viewer can </p> <ol> <li>Record a MCAP file that has the <code>/radar/cube</code> message in it. See Maivin Dataset Recording for details</li> <li>Play the MCAP file in Foxglove Studio. See Playback MCAP with Foxglove Studio for details.</li> <li>In the image panel, the <code>/radar/cube</code> topic should appear under the list of valid image topics. </li> <li>Select the <code>/radar/cube</code> topic</li> <li>Change the color mode to Color Map, and select Turbo for the color map. </li> <li>Leave the value min and value max on auto</li> <li>You can now see the <code>/radar/cube</code> message. </li> </ol>"},{"location":"modules/foxglove/#imu-data-plotting","title":"IMU Data Plotting","text":"<p>To create IMU sensor plots:</p> <ol> <li>Convert bottom left panel to Plot view</li> <li>Click \"Add a Series\"</li> <li>Select <code>/imu</code> as the topic</li> <li>Choose desired parameters (e.g, angular_velocity, x, y, z)</li> <li>Repeat to add additional plot series as needed </li> </ol>"},{"location":"modules/foxglove/#additional-resources","title":"Additional Resources","text":"<p>For more detailed information about Foxglove Studio features, visit the Foxglove Documentation website.</p>"},{"location":"modules/hardware/","title":"Hardware","text":"<p>The Maivin Perception Platform is available in two configurations: Maivin and Raivin.  The Maivin configuration is a vision-only platform while the Raivin configuration includes an integrated radar module.  The Raivin configuration is built on the core Maivin configuration and both run the same software stack.  The Raivin configuration also runs the additional radar and sensor fusion software.</p> <p>The Maivin configuration provides a vision-based perception stack for use in harsh environments, providing an IP66/67 waterproof enclosure and connectors.  The Maivin platform is built on the NXP i.MX 8M Plus processor which includes a 2 TOPS AI accelerator.  The EdgeFirst Perception Middleware leverages the AI-accelerator enabling this vision sensor to be deployed in the field to deliver real-time edge perception applications.</p> <p>The Raivin configuration further extends the perception capabilities of the platform through the addition of the integrated radar module.  The EdgeFirst Perception Middleware is augmented for Raivin configurations with the RadarExp Fusion Model which provides low-level radar data fusion with the vision data.  The low-level radar data is reprensented as the range and doppler data cube, the RadarExp module fuses this data with the vision data to provide a more robust perception stack.  The traditional point-cloud data from the radar is also available for use by the Raivin perception stack, and is especially useful for augmenting the object detection and tracking capabilities through the additional parameters offered by the point cloud data.  </p> <p>Further details on the Fusion Model can be found in the EdgeFirst Perception Developer Guide.  Details of annotating datasets for training can be found in the Deep View Enterprise User Manual.</p>"},{"location":"modules/hardware/#hardware-features","title":"Hardware Features","text":"<ul> <li>Small, optimized form factor for easy deployment for true application testing with enclosure.</li> <li>2 board architecture with Processor Board, and Images Sensor board.  Allows different image sensors to be used.</li> <li>12-24V power input with protection for flexible installations. </li> <li>Flexible communications options including RS-485, Ethernet, and Wi-Fi.</li> <li>M.2 PCIe and USB expansion interface:</li> <li>Wireless LAN modem support</li> <li>Additional AI accelerator support</li> <li>Expansion memory support via SD Card</li> <li>Internal UART console debug interface</li> <li>GNSS receiver</li> <li>Inertial Measurement Unit (IMU)</li> <li>Protected Input/Output for connection to external devices.</li> <li>Rugged waterproof enclosure for outdoor installations. Ready for field installation.</li> <li>M12 waterproof circular connectors for Power/Communications</li> <li>Waterproof RJ45 for Ethernet</li> </ul>"},{"location":"modules/hardware/#hardware-specifications","title":"Hardware Specifications","text":""},{"location":"modules/hardware/#electrical-specifications","title":"Electrical Specifications","text":""},{"location":"modules/hardware/#mechanical-specifications","title":"Mechanical Specifications","text":""},{"location":"modules/hardware/#connectors","title":"Connectors","text":""},{"location":"modules/issues/","title":"Known Issues","text":""},{"location":"modules/issues/#ssl-peer-certificate-or-ssh-remote-key-was-not-ok-during-ostree-pull","title":"\"SSL peer certificate or SSH remote key was not OK\" during <code>ostree pull</code>","text":"<p>If you update the <code>ethernet0</code> network interface without a reboot and try to run an <code>ostree pull</code> command, you may get the following error: <pre><code>error: While fetching https://maivin.deepviewml.com/ostree/summary.sig: [60] SSL peer certificate or SSH remote key was not OK\n</code></pre> If so, reboot the system with the <code>sudo reboot</code> command.  This should address the issue.</p>"},{"location":"modules/networking/","title":"Networking","text":"<p>Torizon for Maivin uses the standard NetworkManager framework for managing system networking.  NetworkManager is used by most Linux distributions and can manage all of the system's networking interfaces, including the optional LTE modem, and be used to configure VPN connections.</p> <p>As noted in the Quick Start, the Maivin can be discovered on a network using the <code>verdin-imx8mp-XXXXXXXX.local</code> hostname where the 'XXXXXXXX' is the eight-digit serial number of the unit.  The serial number can be found on the rear sticker near the connection ports.</p> <p>This chapter documents the most common networking configurations for the Maivin.  For further detailed documentation, please refer to the NetworkManager Documentation.</p>"},{"location":"modules/networking/#network-interfaces","title":"Network Interfaces","text":"<p>The Maivin includes a number of networking interfaces: Ethernet, WiFi, and an optional LTE modem.  The interface names and ports are described in the table below.  The <code>ethernet1</code> and <code>can0</code> interfaces are internal-only interfaces used by the Raivin to communicate with the internal radar module.  The remaining interfaces are user configurable.</p> Name ID Details Gigabit Ethernet ethernet0 RJ-45 port on the rear of the Maivin WiFi Client mlan0 WiFi client interface WiFi AP uap0 WiFi Access Point interface LTE Modem wwan0 Optional LTE Modem using internal m.2 expansion Radar Ethernet ethernet1 Internal 1000BaseT1 interface to the Radar CAN Bus can0 Internal CAN interface to the Radar"},{"location":"modules/networking/#ethernet-setup","title":"Ethernet Setup","text":"<p>The Maivin ships with the Ethernet interface setup as a DHCP client and should automatically get an IP address on a network which provides DHCP.  Otherwise the Maivin will fallback to a link-local IP.  In either scenario, the Maivin can be accessed using the <code>verdin-imx8mp-XXXXXXXX.local</code> hostname.</p> <p>Note</p> <p>The Maivin also supports IPv6 which will be available through a link-local address.  This IPv6 link-local address will be assocated with the <code>verdin-imx8mp-XXXXXXXX.local</code> hostname.</p> <p>Tip</p> <p>If connecting to the Maivin using a Windows PC, either with an SSH client or a web browser, adding the \".local\" suffix is optional.  For Linux or Mac machines, the suffix must be used.</p> <p>Once connected to the Maivin, standard Linux tools are available for listing the network interfaces and their current status.  NetworkManager offers the <code>nmcli</code> command-line tool for querying the status of the interfaces it manages; for example, the <code>nmcli connection</code> command is shown below.  We see the connection names <code>network0</code> and <code>network1</code> which are associated to the network interfaces <code>ethernet0</code> and <code>ethernet1</code>, respectively.  The connection names can be user defined, by default the <code>networkX</code> convention is followed which maps <code>X</code> to the equivalent <code>ethernetX</code> interface.</p> <pre><code>$ nmcli -t connection\nnetwork0:958cc5e3-1bbf-3d64-beeb-020d4414e254:802-3-ethernet:ethernet0\nnetwork1:78c31df4-8c89-31a6-9aeb-d5603e230e4e:802-3-ethernet:ethernet1\n</code></pre> <p>Tip</p> <p>When using <code>nmcli</code>, the -t parameter is used to provide simpler text output.  Without it, the rich console interface is used which requires an appropriate terminal to display correctly.  If you find the output of nmcli visually erroneous, use the -t parameter.</p> <p>Tip</p> <p>The <code>nmcli</code> sub-commands can be abbreviated.  For example, <code>nmcli connection</code> can simply be entered as <code>nmcli c</code>.</p> <p>Other standard Linux tools such as <code>ifconfig</code>, <code>ip</code>, and <code>ethtool</code> are also available.  For example, <code>ifconfig</code> can be used to see the link status and IP address along with other details.</p> <pre><code>$ ifconfig ethernet0\nethernet0 Link encap:Ethernet  HWaddr 00:14:2D:E7:08:E3  \n          inet addr:10.10.41.226  Bcast:10.10.47.255  Mask:255.255.248.0\n          inet6 addr: fe80::1cc:4f8d:9ea2:815/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:2091191 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:251375 errors:2 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:188791038 (180.0 MiB)  TX bytes:329873651 (314.5 MiB)\n          Interrupt:52\n</code></pre>"},{"location":"modules/networking/#static-ip","title":"Static IP","text":"<p>The Maivin can be configured to use a static IP for ethernet using the NetworkManager command-line tool.  You will need to know the desired IP address and subnet mask.  You will also need the gateway and DNS IP address, if required for Internet access.</p> <p>First, you must identify the connection name of the interface you wish to configure with a static IP.  The external gigabit ethernet interface <code>ethernet0</code> has the connection name <code>network0</code> by default, which we can see in the first column of the output.  You will use this connection name in the following commands.</p> <pre><code>$ nmcli -t c\nnetwork0:958cc5e3-1bbf-3d64-beeb-020d4414e254:802-3-ethernet:ethernet0\n</code></pre> <p>The following steps are then followed to configure the static IP address.  Note that the network bitmask (or netmask) is provided in Classless Inter-Domain Routing (CIDR) format.  This lists the numbers of bits to set in the IP address as belonging to the network.  For example, the netmask '255.255.255.0' has the first 24 bits set (each 255 is 8 bits), so the netmask is set using <code>/24</code>.  You may need to adjust this according to your specific network configuration.</p> <ol> <li>Set the static IP address:</li> <li><code>nmcli con mod network0 ipv4.addresses 192.168.1.10/24</code></li> <li>Set the gateway, if required:</li> <li><code>nmcli con mod network0 ipv4.gateway 192.168.1.1</code></li> <li>Set the DNS, if required:</li> <li><code>nmcli con mod network0 ipv4.dns 8.8.8.8</code></li> <li>Set the IPv4 method to manual:</li> <li><code>nmcli con mod network0 ipv4.method manual</code></li> <li>Finally, bring up the new configuration (or sudo reboot the unit):</li> <li><code>nmcli con up network0</code></li> </ol> <p>The device should now be configured with the new static IP address.</p> <p>Warning</p> <p>The internal Ethernet1 interface uses the 192.168.11.0/24 network to communicate with the radar module.  Do not configure the static IP address to reside on this network.</p> <p>Warning</p> <p>The <code>ipv4.addresses</code> element must be configured before switching the IPv4 method to manual.</p> <p>Warning</p> <p>Calling the <code>connection up</code> command while connected through this interface can cause the link to drop.  Reconnect using the new address.  An alternate to the <code>connection up</code> command is to use <code>sudo reboot</code> to restart the device.</p>"},{"location":"modules/networking/#re-enable-dhcp","title":"Re-enable DHCP","text":"<p>To revert the connection to using DHCP you must follow these steps.</p> <ol> <li>Configure automatic (DHCP) mode:</li> <li><code>nmcli con mod network0 ipv4.method auto</code></li> <li>Remove the previous static addresses:</li> <li><code>nmcli con mod network0 ipv4.addresses \"\"</code></li> <li>Remove the previous gateway:</li> <li><code>nmcli con mod network0 ipv4.gateway \"\"</code></li> <li>Remove the previous DNS:</li> <li><code>nmcli con mod network0 ipv4.dns \"\"</code></li> <li>Activate the changes:</li> <li><code>nmcli con up network0</code></li> </ol>"},{"location":"modules/networking/#mixed-static-and-dhcp","title":"Mixed Static and DHCP","text":"<p>NetworkManager allows mixed static and DHCP configurations.   If you configure <code>ipv4.method auto</code> you can still configure some of the other IPv4 parameters statically.</p>"},{"location":"modules/networking/#wifi-client-setup","title":"WiFi Client Setup","text":"<p>Maivin units with WiFi can be configured to connect to a WiFi Access Point (AP).  Refer to the next section if you instead want to use your Maivin as a WiFi AP.</p> <p>Warning</p> <p>FCC regulations require special certifications for colocated transmitters (a device with multiple RF transmitters).  Maivin and Raivin are not currently certified for colocated transmitters so users must not enable more than one of the WiFi, Modem, or Radar without first receiving FCC certification.  Refer to regulatory boards in your jurisdiction for your relevant regulations.</p> <p>NetworkManager handles the WiFi client configuration.  Follow these steps to connect to an AP, you will need to know the AP Service Set Identifer (SSID, usually the network name) and the password, if required.</p> <ol> <li>First scan for available WiFi networks to join:</li> <li><code>nmcli device wifi list</code></li> <li>Connect to the desired WiFi network using the SSID:</li> <li><code>nmcli -a device wifi connect &lt;SSID&gt;</code></li> <li>Enter the password, if needed, when prompted.</li> <li>Confirm the <code>mlan0</code> interface is up and has received an IP address:</li> <li><code>ifconfig mlan0</code></li> </ol>"},{"location":"modules/networking/#wifi-ap-setup","title":"WiFi AP Setup","text":"<p>Maivin units with WiFi can be configured as an AP allowing client devices to connect to the Maivin.  Instead of using NetworkManager, we use the Host AP daemon (hostapd).</p> <p>Warning</p> <p>FCC regulations require special certifications for colocated transmitters (a device with multiple RF transmitters).  Maivin and Raivin are not currently certified for colocated transmitters so users must not enable more than one of the WiFi, Modem, or Radar without first receiving FCC certification.  Refer to regulatory boards in your jurisdiction for your relevant regulations.</p> <p>WiFi AP mode is configured using the <code>hostapd</code> service in Linux.  We describe a common WiFi AP configuration; for more advanced setup, please refer to the hostapd documentation.</p> <p>To enable the Maivin Access Point with default configurations simply enable the <code>hostapd</code> service.</p> <pre><code>sudo systemctl enable hostapd\nsudo systemctl start hostapd\n</code></pre> <p>The Maivin ships with a default hostapd configuration which can be modified for your needs.  The following shows the base configuration which ships with Maivin. The full list of configuration options is documented in the hostapd.conf reference file.</p> <pre><code>ssid=Maivin\nwpa_passphrase=maivin\nown_ip_addr=10.10.10.1\ninterface=uap0\n\ncountry_code=CA\nhw_mode=a\nchannel=40\n\nieee80211ac=1\nieee80211n=1\nieee80211w=2\nwmm_enabled=1\n\nauth_algs=1\nwpa=2\nwpa_key_mgmt=WPA-PSK\nrsn_pairwise=CCMP\nwpa_pairwise=CCMP\nsae_require_mfp=1\n\nht_capab=[LDPC][HT40+][GF][SHORT-GI-20][SHORT-GI-40][TX-STBC][DSSS_CCK-40]\n</code></pre> <p>Warning</p> <p>Please make sure to change the default password before enabling WiFi AP mode!</p> <p>The WiFi AP network configuration file is found under <code>/etc/systemd/network/hostapd.network</code> and is managed by systemd.  The following is the default configuration.  The full list of configuration options is documented in the systemd network manual.</p> <pre><code>[Match]\nName=wlan0 uap0\nWLANInterfaceType=ap\n\n[Network]\nAddress=10.10.10.1/24\nDHCPServer=true\nIPMasquerade=yes\nIPForward=ipv4\n\n[DHCPServer]\nPoolOffset=100\nPoolSize=100\n</code></pre> <p>Warning</p> <p>If you change the network address in <code>hostapd.network</code>, make sure it matches the address in <code>hostapd.conf</code>!</p>"},{"location":"modules/networking/#lte-modem-setup","title":"LTE Modem Setup","text":"<p>Maivin provides an m.2 expansion port which can be used to add an LTE modem to the device.  We offer Maivin and Raivin units pre-configured with an LTE modem and SIM card or a modem can be added by customers themselves following the instructions at the end of this section.</p> <p>Warning</p> <p>FCC regulations require special certifications for colocated transmitters (a device with multiple RF transmitters).  Maivin and Raivin are not currently certified for colocated transmitters so users must not enable more than one of the WiFi, Modem, or Radar without first receiving FCC certification.  Refer to regulatory boards in your jurisdiction for your relevant regulations.</p>"},{"location":"modules/networking/#modem-configuration","title":"Modem Configuration","text":"<p>The modem in the Maivin is configured through ModemManager, which is a companion framework to NetworkManager but specializing in modem configuration.  Before you start, you need to ensure the modem is installed into the Maivin along with the SIM card and that you have the correct Access Point Name (APN) configuration string from your LTE carrier.</p> <p>We describe the common modem configuration in this section.  For further details of the modem features and configuration, please refer to the ModemManager Documentation.</p>"},{"location":"modules/networking/#configure-m2-port","title":"Configure m.2 Port","text":"<p>To configure the modem, we first need to ensure the m.2 expansion port is configured for the correct operating mode.  The expansion port supports PCI and USB operating mode and the correct one needs to be configured in the device tree.  Most modems use USB.  The device tree is configured through the <code>/etc/overlays.txt</code> file, so please make sure the entry <code>maivin2-m2usb.dtbo</code> is listed.</p> <pre><code>$ cat /etc/overlays.txt\nfdt_overlays=maivin2.dtbo maivin2-os08a20.dtbo maivin2-m2usb.dtbo\n</code></pre> <p>Warning</p> <p>The <code>maivin2.dtbo</code> overlay must always be present first.  Care must be taken when adjusting the device tree overlays as errors will cause boot failures!</p>"},{"location":"modules/networking/#verify-modem-found","title":"Verify Modem Found","text":"<p>After confirming the correct device tree overlays are configured and rebooting the Maivin, you can verify the modem has been found.  For USB modems, the <code>lsusb</code> command is used, in the example below we see a Telit LN920 modem has been found.</p> <pre><code>$ lsusb\nBus 002 Device 002: ID 1bc7:1060 Telit Wireless Solutions LN920\n</code></pre>"},{"location":"modules/networking/#configure-networking","title":"Configure Networking","text":"<p>With the m.2 expansion and modem configured and detection verified the networking can now be configured.</p> <p>First we must enable the ModemManager service which is disabled by default on Maivin.</p> <pre><code>sudo systemctl enable ModemManager\nsudo systemctl start ModemManager\n</code></pre> <p>Next we can confirm that the modem and SIM card are found.</p> <pre><code>$ mmcli -m 0\n  ----------------------------------\n  General  |                   path: /org/freedesktop/ModemManager1/Modem/0\n           |              device id: 7d04c0ee5027689a762768e20387443c2eb574ea\n  ----------------------------------\n  Hardware |           manufacturer: Telit\n           |                  model: LN920A6-WW\n           |      firmware revision: M0L.010002\n           |         carrier config: default\n           |           h/w revision: 1.10\n           |              supported: gsm-umts, lte\n           |                current: gsm-umts, lte\n           |           equipment id: 352214163072532\n&lt;additional output removed&gt;\n</code></pre> <p>Note</p> <p>If <code>mmcli -m 0</code> returns the error \"error: couldn't find the ModemManager process in the bus\", then you need to first start the service using <code>sudo systemctl start ModemManager</code>.</p> <p>Note</p> <p>If <code>mmcli -m 0</code> returns the error \"error: couldn't find modem\", then wait a few minutes before trying again.  Modem startup can take a while.  If the modem is still not found, please contact support.</p> <p>Now that the modem is properly detected by ModemManager, we switch to NetworkManager to configure the network.</p> <p>Warning</p> <p>Take careful note of the commands, as <code>mmcli</code> for ModemManager and <code>nmcli</code> for NetworkManager look very similar.</p> <ol> <li>First we create a new NetworkManager connection for the cellular modem and provide the APN string which will be provided to you by your service provider: <code>nmcli connection add type gsm ifname \"*\" con-name \"Cellular Connection\" autoconnect yes gsm.apn &lt;APN_FROM_CARRIER&gt;</code></li> <li>Next you can bring up the newly created connection: <code>nmcli connection up \"Cellular Connection\"</code></li> <li>Then you can verify the connection is up: <code>nmcli -c no connection show</code></li> </ol> <p>Once properly configured the <code>wwan0</code> interface can also be queried through <code>ifconfig</code>.</p>"},{"location":"modules/networking/#modem-installation","title":"Modem Installation","text":"<p>The modem installation can be performed by customers by following these instructions.  If your modem and service provide require physical SIM card installation that is also possible during installation.</p> <p>Warning</p> <p>Installing an LTE modem is an advanced configuration which requires opening up the Maivin.  Extreme caution should be followed during this procedure.  We suggest ordering Maivin or Raivin units with the LTE option pre-configured.</p>"},{"location":"modules/networking/#radar-networking","title":"Radar Networking","text":"<p>The <code>ethernet1</code> and <code>can0</code> interfaces are reserved for internal communications with the radar module on Raivin configurations.  Refer to the Radar page for details.</p>"},{"location":"modules/publishing/","title":"Publishing","text":""},{"location":"modules/quickstart/","title":"Quick Start","text":"<p>This article will walk you through the Raivin setup and then lead you to resources for using additional features.</p>"},{"location":"modules/quickstart/#unboxing","title":"Unboxing","text":"<p>The Maivin/Raivin box contains the following items:</p> <ul> <li>The Maivin/Raivin vision module</li> <li>A five-meter power cable, M12 circular connector (male) to 2.1mm x5.5 barrel adapter (female)</li> <li>Box with power adapters<ul> <li>Power-adapter with interchangeable plugs.  2.1mm x 5.5mm barrel adapter (male) connector.</li> <li>Interchangeable plugs for the following regions:<ul> <li>NEMA 1-15P (Type A) (North America)</li> <li>CEE-7/16 Alternative II \"Europlug\" (Type C) (Europe)</li> <li>AS/NZS 3112, ungrounded (Type I) (Australia / Oceania)</li> <li>BS 1363 (Type B) (British) wall adapter</li> </ul> </li> </ul> </li> <li>Desktop tripod</li> </ul>"},{"location":"modules/quickstart/#connecting-the-device","title":"Connecting the Device","text":"<p>If you wish to mount your Raivin to the tripod, it should be done now before connecting any of the other cables.  There are three mount points on the bottom of the device and one on each side.  Screw the tripod into the center bottom mount point.</p> <p>Next, connect a standard Category 5 network cable (not included) from your shared network into the device.</p> <p>Optionally, you can connect an antenna to the SMA connecter on the top-right corner of the back of the device to enhance GPS reception.</p> <p>Chose the proper power adapter plug from your region and connect it to the power cable.  Then connect the M12 connector of the power cable to the connection at the back of the module, making sure to align the tab at the top of the connecter to its corresponding slot. </p> <p>The device should boot up as soon as it is connected.  A blue light above and to the right of the power connecter should start blinking. Raivin showing network connection (left), eight-digit ID number (middle), tripod connected (bottom), and power connection with blue status light on (right)</p> <p>Danger</p> <p>The Raivin may get hot during operation.  Do not handle while operating.  Temperature can be measured with the <code>cat /dev/carrier_temp</code> command, which will output device temperature in millidegree Celsius.</p>"},{"location":"modules/quickstart/#on-boot-up","title":"On Boot Up","text":"<p>The Raivin will have an eight-digit number on the back of the device.  This is the ID number.  The hostname of the Raivin will be \"verdin-imx8mp-ID.local\", which is advertised over Multicast Domain Name System (mDMS).  For the steps below, the eight-digit number is \"15141029\".  The device will have a hostname of <code>verdin-imx8mp-15141029.local</code>.  This hostname can be used to connect to the device over SSH and HTTP.</p> <p>Tip</p> <p>On Windows machines, you will not need to add the '.local' suffix.</p> <p>The Raivin has a web interface that can be connected to via both HTTP and HTTPS by entering <code>http://verdin-imx8mp-&lt;id&gt;.local</code>.  On first connection to the web interface, you will get a \"Your connection is not private\" warning.  This is expected and nothing to worry about -- the HTTPS connection needs a SSL certificate which the vision module does not have.  Click the \"Advanced\" button, and then \"Proceed to\" link. </p> <p>After all that, you should see the Raivin Main Page. . If you have a Maivin, you will see the Maivin Main Page instead: </p> <p>From here, we recommend that you check out the Segmentation View page by clicking the \"Segmentation View\" card.  For a Raivin, it looks as so:  and for a Maivin: </p> <p>Note</p> <p>The ultra-short model included in this release was trained for fixed camera and mostly tested indoors.</p>"},{"location":"modules/radar/","title":"Radar Module","text":"<p>The Raivin configuration includes an integrated DRVEGRD-169 radar module from smartmicro.  The radar module is internally connected to the Raivin which provides power and data communications interfaces.  The primary data and control interface uses the CAN bus protocol.  The radar module is also connected through an internal Gigabit Ethernet interface which is used to transmit the radar data cube used by the RadarExp Fusion model.</p>"},{"location":"modules/radar/#specifications","title":"Specifications","text":"<p>The DRVEGRD 169 radar module is a 79GHz radar sensor for multiple automotive applications that features 4D/PxHD technology.  The sensor\u2019s antenna aims at ultra-short, short and medium range applications with a very wide, horizontal angular coverage of 140\u00b0.  A full set of the radar module specifications can be found here.</p>"},{"location":"modules/radar/#networking","title":"Networking","text":"<p>The radar module is connected to the Raivin using two networking interfaces.  The primary interface is the <code>can0</code> interface which is used to configure the radar module and receive the radar point-cloud.  This interface is required for to control the radar module.  The secondary interface is the <code>ethernet1</code> interface which is used to transmit the radar data cube data to the Raivin.  This interface is required to provide input to the RadarExp Fusion model, but is not required for the radar module to function in point-cloud mode.  The radar data cube generates about 300Mb/s of data which is transmitted to the Raivin for processing.</p>"},{"location":"modules/radar/#can-configuration","title":"CAN Configuration","text":"<p>The CAN bus interface is managed through the systemd networking framework using the configuration file <code>/etc/systemd/network/can0.network</code>.  The CAN bus interface will be pre-configured by Torizon for Maivin but is covered in this section.</p> <pre><code>[Match]\nName=can0\n\n[CAN]\nBitRate=500K\n</code></pre> <p>The <code>can0</code> interface is configured for 500kbps.  The Radar Publishing Service manages the radar configuration and reading of the point-cloud data over CAN and publishing the results over Zenoh.  Refer to the Radar Publishing Service for details.</p>"},{"location":"modules/radar/#ethernet-configuration","title":"Ethernet Configuration","text":"<p>The radar module streams the low-level radar data cube over an ethernet interface which is internally connected to the Raivin's <code>ethernet1</code> interface.  The connection is managed through the systemd network framework using the configuration file <code>/etc/systemd/network/ethernet1.network</code> along with the <code>ethernet1-master.service</code> systemd service which handles configuring the automotive ethernet PHY (1000Base-T1) as the connection master.  The <code>ethernet1-master.service</code> should be enabled using <code>sudo systemctl enable ethernet1-master</code> if not already enabled.</p> <pre><code>[Match]\nName=ethernet1\n\n[Network]\nAddress=192.168.11.17/24\n</code></pre>"},{"location":"modules/recording/","title":"MCAP Recording Service","text":""},{"location":"modules/recording/#overview","title":"Overview","text":"<p>The Recorder Service enables you to capture and store ROS2 topic data in MCAP format, providing a comprehensive recording solution for your Raivin system. This service acts as a data historian, collecting published topics from various services for later analysis and playback.</p>"},{"location":"modules/recording/#how-it-works","title":"How It Works","text":"<p>When active, the Recorder Service:</p> <ul> <li>Logs specified topics</li> <li>Compiles data into an MCAP-format recording</li> <li>Automatically saves the file upon service termination</li> <li>Provides real-time recording status</li> </ul>"},{"location":"modules/recording/#using-the-recorder","title":"Using the Recorder","text":"<p>The MCAP Recording Service is managed on its own visualization page, which can be accessed by clicking the \"MCAP\" card on the Raivin Main Page or by typing <code>https://&lt;hostname&gt;/mcap</code> in your browser window.  At the top of page, under the ribbon, we have the Recording Toggle, which will start or end an MCAP recording.  We also have the \"Live Mode\" button, which will take the Raivin out of \"Replay Mode\" and back to \"Live Mode\".</p> <p>Under this, we have the listing of MCAP files currently the MCAP recording directory.  This directory is noted at the top of the list -- in the above image, it is <code>/media/DATA</code>.</p> <p>For each MCAP file, the following elements and information exist, starting from left to right:</p> <ul> <li>A playback button. Clicking this will put the device into \"Replay Mode\", replaying the sensor information from this MCAP file.</li> <li>The filename of the MCAP</li> <li>The size of the MCAP, in MB</li> <li>The creation date and time of the MCAP</li> <li>The video length of the MCAP</li> <li>Four Action buttons, which are:<ul> <li>The \"Details\" button, which shows information about the topics recorded in the MCAP file (see below).</li> <li>The greyed-out and currently non-functionality \"Upload\" button.</li> <li>The \"Download\" button, which will download the MCAP file to your local machine.</li> <li>The \"Delete\" button, which will remove the MCAP file.</li> </ul> </li> </ul>"},{"location":"modules/recording/#starting-a-recording","title":"Starting a Recording","text":"<p>To start a recording, simply click the \"Recording\" toggle to begin capturing data.  Note the the red text describing that recording is in progress with the filename of the recording.  Also notice the the Recording notification on the top ribbon is now on. <pre><code>It may take up to 30 seconds for a recording to start, depending on topic tracked.\n</code></pre> To stop recording, click the \"Recording\" toggle a second time.</p>"},{"location":"modules/recording/#managing-recordings","title":"Managing Recordings","text":"<p>Once a recording is complete, you can see the size in MB and duration in seconds of the captured data on the MCAP page.  You can also get more detailed metadata, such as the topics recorded, by clicking the \"Details\" button for the MCAP recording.  At the bottom of the \"File Details\" modal, there is a \"Close\" button to close the modal. <pre><code>MCAP recording file names are saved in `hostname_YYYY_mm_DD_HH_MM_SS.mcap` format, where:\n- _hostname_ is the hostname of the device, e.g. `verdin-imx8mp-071744901`\n- _YYYY_mm_DD_ is the zero-padded year, month, and day that the recording was started\n- _HH_MM_SS_ is the UTC time the recording start in 24-hour notation\n</code></pre> Any MCAP recording can be deleted by clicking its \"Delete\" button and confirming that you wish to delete the button.</p>"},{"location":"modules/recording/#downloading-and-analysis","title":"Downloading and Analysis","text":"<p>Click the \"Download\" button to save the MCAP file to your PC.  You can also use a SSH client to copy files off the Raivin.  Once the MCAP recording is saved on to your PC, you can use an MCAP reader such as Foxglove Studio to analyze the recorded data.</p>"},{"location":"modules/recording/#configuration","title":"Configuration","text":"<p>The Recorder Service can have the following settings configured: - Which topics to record - Location of the recording file - Recording compression</p> <p>These settings can be configured in the MCAP Recorder Settings Page.</p>"},{"location":"modules/release_notes/","title":"Release Notes","text":""},{"location":"modules/release_notes/#version-202501","title":"Version 2025.01","text":"<p>This is the first official release of the Maivin Perception Platform.</p> <p>For Maivin users this software release is a major upgrade which unifies the Torizon for Maivin branches of Raivin and Maivin into a single software release.  As part of this release the version naming has been updated to follow the the YEAR.MONTH.PATCH format instead of using the Torizon version number as the base.  The YEAR and MONTH refer to the data of the initial release of the software and the PATCH will be included for incremental patch release within this release cycle.  The upstream Torizon OS version number is documented in the release notes.</p> <p>This release includes a major update to the WebUI interface of the Maivin Perception Platform.  The new interface provides a collection of panels that can be used to monitor and control the platform.  The primary page for the Maivin is the \"Segmentation View\" page, where the Raivin provides a combined segmentation view and occupancy grid panel.  The WebUI also provides a page for only the occupancy grid.  Configuration pages have been added allowing the user to configure various aspects of the Maivin platform. Currently, configuration is focused on the EdgeFirst Middleware services but future updates plan to add networking and other configuration options.</p>"},{"location":"modules/release_notes/#edgefirst-models","title":"EdgeFirst Models","text":"<ul> <li>ModelPack for Detection and Segmentation</li> <li>RadarExp Fusion Model</li> </ul>"},{"location":"modules/release_notes/#edgefirst-packages","title":"EdgeFirst Packages","text":"<ul> <li>Camera</li> <li>Radarpub</li> <li>Model</li> <li>Fusion</li> <li>IMU</li> <li>NavSat</li> <li>Recorder</li> <li>Playback</li> <li>WebUI</li> <li>Web Server</li> </ul>"},{"location":"modules/release_notes/#system-packages","title":"System Packages","text":"<ul> <li>Torizon 6.8.1</li> <li>Linux 5.15.148</li> </ul>"},{"location":"modules/replay/","title":"MCAP Replay Service","text":""},{"location":"modules/replay/#overview","title":"Overview","text":"<p>The Replay Service allows users to play back previously recorded MCAP files, enabling detailed analysis of MCAP data. This service provides flexible playback options and integrates with live fusion and model data for comprehensive testing and validation.</p>"},{"location":"modules/replay/#how-it-works","title":"How It Works","text":"<p>The Replay Service offers: - Playback of recorded MCAP files - Hybrid mode combining recorded and live data - Real-time status monitoring - Seamless integration with live system operations</p>"},{"location":"modules/replay/#using-the-replay-service","title":"Using the Replay Service","text":"<p>The Replay Service is built into the MCAP Recorder Page.  You can visit the page by clicking the \"MCAP\" card on Raivin Main Page or typing <code>https://&lt;hostname&gt;/mcap</code> in your web browser.</p>"},{"location":"modules/replay/#starting-playback","title":"Starting Playback","text":"<ol> <li>Locate your desired MCAP file in the file list</li> <li>Click the \"Play\" button (\u25b6\ufe0f) next to the file</li> <li>In the playback options dialog, choose your preferred settings:</li> <li>Fusion Source: Choose between Live or MCAP data for the post-processed radar topics</li> <li>Model Source: Choose between Live or MCAP data for the post-processed image topics </li> <li>Click \"Start\" to begin playback</li> </ol> <p>During playback, the currently playing file will be highlighted and marked as \"Now Playing\".  The State Indicator at the right in the top ribbon also notes we are \"Replay Mode\". </p> <p>Now that the device is in \"Replay Mode\", the inputs to the Segmentation, Occupancy, GPS, and IMU pages will come from the MCAP file instead of from the live sensors.</p>"},{"location":"modules/replay/#playback-controls","title":"Playback Controls","text":"<ul> <li>Click the \"Live Mode\" button to return to displaying live sensor data. </li> <li>Click the stop button (\u23f9\ufe0f) on the playing file to end playback    <pre><code>Stopping a file from replaying will not put the device back into \"Live Mode\".  You must click the \"Live Mode\" button to return to \"Live Mode\".\n</code></pre> <pre><code>When a file is playing, you cannot play another file until you stop the current playback.  Once you stop a file, you can start playing another file by clicking its \"Play\" button.\n</code></pre></li> </ul>"},{"location":"modules/replay/#hybrid-mode","title":"Hybrid Mode","text":"<p>The Replay Service supports a hybrid mode where you can: - Play back recorded sensor data while using live fusion and/or model data  - Use recorded data for some systems while maintaining live data for others - Mix and match recorded and live data sources based on your testing needs</p>"},{"location":"modules/replay/#status-monitoring","title":"Status Monitoring","text":"<p>This section describes the various states the system can be in and how they are reported.</p>"},{"location":"modules/replay/#system-status-bar","title":"System Status Bar","text":"<p>The status bar provides real-time information about your system's operational state and service health.</p>"},{"location":"modules/replay/#status-indicator","title":"Status Indicator","text":"<p>Located in the top-right corner, the status indicator shows: - Live Mode (Green): System is operating with live data - Replay Mode (Blue): System is playing back an MCAP file - Degraded Mode (Amber): Some services are not operating optimally - Critical Mode (Red): Critical services are not functioning</p>"},{"location":"modules/replay/#recording-indicator","title":"Recording Indicator","text":"<p>A pulsing red circle appears when the system is actively recording. </p>"},{"location":"modules/replay/#service-status","title":"Service Status","text":"<p>Click the information (\u2139\ufe0f) button to view detailed service status, which shows the individual service states (Running/Stopped). </p>"},{"location":"modules/ssh/","title":"SSH","text":"<p>This section describes how to SSH into a Raivin using the default \"torizon\" account, and some basic commands you can use to provide some basic information about your Raivin if you have found an issue.</p> <p>Warning</p> <p>The \"torizon\" account is effectively the root account of the Maivin device.  Using this account can cause irreparable harm to the software internals of the device. </p> <p>Warning</p> <p>As part of the initial SSH session, you will be asked to change the default password.  PLEASE RECORD THIS PASSWORD!  If you forget it, you will be unable to SSH into the device without a reinstall.  </p> <p>You need an SSH client (OpenSSH, PuTTY, etc.) to SSH into the Maivin.</p>"},{"location":"modules/ssh/#the-intial-ssh-session","title":"The Intial SSH Session","text":"<p>First, verify that your Maivin is turned on and connected to the network.  You can follow the Quick Start instructions to get the hostname of the device, which for the examples in this section will be <code>verdin-imx8mp-06976895.local</code>.</p> <p>Tip</p> <p>Windows does not need the <code>.local</code> hostname suffix.</p> <p>In a command prompt, ping the Maivin's hostname.  If the Maivin does not reply, please confirm the Maivin is powered on and connected to the network.</p> <p>Once the ping is successful, use your SSH client to connect to the 'torizon' account on the Maivin's hostname.  For a command-line SSH such as OpenSSH, this should look as:</p> <p><pre><code>ssh torizon@Maivin-hostname\n</code></pre> For example:  On the first log of a new or updated Maivin, you may get the \"authenticity of host\" error.  This is expected, and type \"yes\" to continue connecting.</p> <p>On a new Raivin, it will ask you to change the default password to a new password.  Re-enter the 'torizon' password, and then enter a new password twice.  After you change the default password, the SSH session will close so you can re-login with the new password. </p> <p>Once you've finished those steps, the next and all future SSH sessions should look as follows: </p> <p>You are now at the Linux command-line prompt for the Raivin!</p>"},{"location":"modules/ssh/#standard-debugging-commands","title":"Standard Debugging Commands","text":"<p>The version of Raivin you are running can be found using <code>ostree admin status</code>, <pre><code>$ ostree admin status\n* torizon e56b949928e920ea6045b794315912eec30ae7ae4aa157a2c4e96b4fd0b9b872.0\n    Version: 2025.01rc1\n    origin refspec: torizon/maivin/testing\n  torizon a5454f3490fbc0463c7ec076416049d1bc0ec93709ded78c988bd17430054bc5.0 (rollback)\n    Version: develop-24\n    origin refspec: torizon/maivin/develop\n</code></pre> and in the <code>/etc/os-release</code> file. <pre><code>$ cat /etc/os-release\nID=torizon-maivin\nNAME=\"Torizon for Maivin\"\nVERSION=\"2025.01rc1+build.2 (kirkstone)\"\nVERSION_ID=2025.01rc1-build.2\nPRETTY_NAME=\"Torizon for Maivin 2025.01rc1+build.2 (kirkstone)\"\nDISTRO_CODENAME=\"kirkstone\"\nBUILD_ID=\"2\"\nANSI_COLOR=\"1;34\"\nVARIANT=\"Maivin\"\n</code></pre> The status of a specific service can be determined by running <code>systemctl status &lt;service name&gt;</code>.  For example, we can see the status of the camera service: <pre><code>$ systemctl status camera\n\u25cf camera.service - Maivin Camera Service\n     Loaded: loaded (/usr/lib/systemd/system/camera.service; enabled; vendor preset: enabled)\n     Active: active (running) since Wed 2025-01-29 10:31:34 CST; 6h ago\n    Process: 17955 ExecStartPre=/bin/sleep 30 (code=exited, status=0/SUCCESS)\n   Main PID: 18324 (camera)\n      Tasks: 21 (limit: 3772)\n     Memory: 25.2M\n     CGroup: /system.slice/camera.service\n             \u2514\u2500 18324 /usr/bin/camera\n\nJan 29 10:31:04 verdin-imx8mp-15141029 systemd[1]: Stopped Maivin Camera Service.\nJan 29 10:31:04 verdin-imx8mp-15141029 systemd[1]: Starting Maivin Camera Service...\nJan 29 10:31:34 verdin-imx8mp-15141029 systemd[1]: Started Maivin Camera Service.\n</code></pre> The logs can be viewed using the <code>journalctl</code> command, though this will print all of the logs.  Specific services can be viewed with the <code>-u</code> option. <pre><code>journalctl -u camera | more\nJan 29 10:28:58 verdin-imx8mp-15141029 systemd[1]: Stopping Maivin Camera Service...\nJan 29 10:28:59 verdin-imx8mp-15141029 systemd[1]: camera.service: Deactivated successfully.\nJan 29 10:28:59 verdin-imx8mp-15141029 systemd[1]: Stopped Maivin Camera Service.\nJan 29 10:28:59 verdin-imx8mp-15141029 systemd[1]: Starting Maivin Camera Service...\nJan 29 10:29:29 verdin-imx8mp-15141029 systemd[1]: Started Maivin Camera Service.\n</code></pre> To exit the SSH session, type <code>exit</code>.</p>"},{"location":"modules/ssh/#secure-copy","title":"Secure Copy","text":"<p>We can use secure copy (SCP) to move files to and from the device.  Basic command usage is: <pre><code>scp &lt;source file&gt; &lt;destination file&gt;.\n</code></pre> To describe files on the Raivin, both source and destination, they are formatted as: <pre><code>torizon@verdin-imx8mp-&lt;id&gt;:/absolute/path/filename\n</code></pre> The path is not needed for files in the <code>torizon</code> home directory <code>/home/torizon</code>, and relative paths from <code>/home/torizon</code> can be used.</p> <p>If the destination file will be named the same as a the source file, a period <code>.</code> can be used instead.</p> <p>For example, if we want to upload the \"test.mcap\" file to the device from the current working directory on our local machine to remote device <code>verdin-imx8mp-15141029</code>, we can run the command: <pre><code>scp test.mcap torizon@verdin-imx8mp-15141029:.\n</code></pre> This would copy the file to <code>/home/torizon/test.mcap</code>.</p> <p>To copy an MCAP recording from the <code>/media/DATA/</code> directory on our device, we could use the following command: <pre><code>scp torizon@verdin-imx8mp-15141029:/media/DATA/verdin-imx8mp-15141029_2025_01_29_14_35_23.mcap .\n</code></pre></p>"},{"location":"modules/updates/","title":"Software Updates","text":"<p>The Maivin platform runs a Linux operating system which is based on the Torizon distribution.  The Maivin version of the distribution is refered to as \"Torizon for Maivin\" (TfM).  This TfM distribution uses OSTree to manage software udpates, this updates both the Linux operating system including the kernel, drivers, and core system packages as well as the EdgeFirst Middleware which provides the perception stack.</p> <p>The version naming for the TfM distribution follows the YEAR.MONTH.PATCH format.  The YEAR and MONTH refer to the data of the initial release of the software and the PATCH is the incremental patch release within this release cycle.  The upstream Torizon OS version number is documented in the release notes.</p>"},{"location":"modules/updates/#ostree","title":"OSTree","text":"<p>OSTree is a tool that provides a way to manage the filesystem of a Linux system as a series of snapshots.  Each snapshot is a complete filesystem image that can be booted into.  The system can be updated by switching to a new snapshot.  This allows for atomic updates where the system is either in the old state or the new state.  If the update fails the system can be rolled back to the previous snapshot.</p> <p>The TfM OSTree repository uses branches to manage the different versions of the software.  Three core branches are used:</p> <ul> <li>torizon/maivin/release</li> <li> <p>The release branch is the stable branch that is used for production systems.  This branch tracks the latest YEAR.MONTH.PATCH release.</p> </li> <li> <p>torizon/maivin/testing</p> </li> <li> <p>The testing branch is used for testing new software releases.  This branch tracks the latest YEAR.MONTH.PATCHrcX release candidates.</p> </li> <li> <p>torizon/maivin/develop</p> </li> <li>The dev branch is used for development of new features.  This branch is updated frequently and is likely to contain breaking or undocumented changes.  This branch should only be used by developers working on the platform.</li> </ul> <pre><code>It is recommended that users should only pull ostree updates from the Release branch.\n</code></pre> <p>Other branches may be used for specific purposes such as tracking major changes to the system such as a new major Torizon update or similar updates that have a high impact to the overall system.</p> <p>The currently booted TfM operating system release information can be read from the <code>/etc/os-release</code> file.</p> <pre><code>cat /etc/os-release\n</code></pre>"},{"location":"modules/updates/#updating-the-system","title":"Updating the System","text":"<p>The TfM system can be updated using the OSTree client.  The client is a command-line utility that can be used to manage the system snapshots.</p> <p>To update the release: 1. Access your Maivin through SSH 2. Pull the latest Maivn release update with the <code>ostree pull</code> command:     <pre><code>torizon@verdin-imx8mp-15141091:~$ sudo ostree pull maivin:torizon/maivin/release\n1 delta parts, 2 loose fetched; 481 KiB transferred in 2 seconds; 0 bytes content written\n</code></pre> 3. Deploy the pulled release with the <code>ostree admin deploy</code> command:     <pre><code>torizon@verdin-imx8mp-15141091:~$ sudo ostree admin deploy torizon/maivin/release\nnote: Deploying commit 71815d485d8237adedfd4cc041511d43406e9861b5e9107dba7e5b3e217442c4 which contains content in /var/local that will be ignored.\nCopying /etc changes: 14 modified, 12 removed, 21 added\nTransaction complete; bootconfig swap: yes; bootversion: boot.1.1, deployment count change: 0\nFreed objects: 6.7?MB\n</code></pre> 4. Reboot the Maivin with the <code>sudo reboot</code> command.</p>"},{"location":"modules/updates/#status","title":"Status","text":"<p>The current ostree status can be viewed using the following command:</p> <pre><code>ostree admin status\n</code></pre> <p>This will show the currently loaded snapshot and the rollback snapshot if one is available.  Whenever OStree is updated, the previous version will be available as a snapshot allowing rollbacks if the update fails or causes issues.</p>"},{"location":"modules/updates/#changelogs","title":"Changelogs","text":""},{"location":"modules/updates/#update-rollback","title":"Update Rollback","text":""},{"location":"modules/updates/#ostree-repository","title":"OSTree Repository","text":"<p>The Torizon for Maivin OSTree repository is hosted by Au-Zone Technologies and is available here.  This repository is accessed through the OSTree client  using the configuration file under <code>/etc/ostree/remotes.d/maivin.conf</code>.</p>"},{"location":"modules/updates/#further-reading","title":"Further Reading","text":"<p>For further information about how the Torizon platform uses ostree, refer to the official Toradex documentation Torizon In-Depth OSTree.</p>"},{"location":"modules/walkthrough/","title":"Web UI Walkthrough","text":"<p>This chapter will walk you through the Raivin's and Maivin's Web User Interface (WebUI).</p>"},{"location":"modules/walkthrough/#the-main-page","title":"The Main Page","text":"<p>The Main Page of the Raivin web interface should look as follows: </p> <p>The Main Page for the Maivin looks slightly different: </p> <p>There are five cards on the Main Page that link to the Visualization pages:</p> <ul> <li>GPS: This page displays a map with the current location of the device, along with GPS co-ordinates.</li> <li>IMU: This page displays the 3D orientation of the device with current pitch, yaw, and roll values.</li> <li>MCAP: This page contains the visual/radar recordings management interface.</li> <li>Segmentation View: This page shows the camera with running segmentation and/or detection pipeline.  For Raivins equipped with a radar module, it will also show the radar grid.</li> <li>Occupancy Grid: (Raivin only) This will show the radar grid.</li> </ul>"},{"location":"modules/walkthrough/#the-top-ribbon","title":"The Top Ribbon","text":"<p>The ribbon at the top of the Raivin web interface is available on every page of the web interface.  The following three elements are available on every page of the Raivin web interface.</p> <ol> <li>On the left, the \"Home\" button with the Au-Zone icon, which will return the user to the Main Page.</li> <li>In the middle, the title of the current page.</li> <li>The farthest rightmost button, with the gear icon, is the Settings Buttons and will take you to the Settings Page.</li> </ol> <p>On the right side of the ribbon, we have a grouping of two indicators an a dropdown menu.  These items are only shown on the Main Page and the pages clicked from the five cards on the Main Page.  These elements, from left to right, are:</p> <ol> <li>The Recording Indicator, shown as a red circle when recording from the sensors.</li> <li>The System Status Indicator.</li> <li>The Status Information Dropdown button.</li> </ol>"},{"location":"modules/walkthrough/#system-status-indicator","title":"System Status Indicator","text":"<p>Mousing over the System Status Indicator field will give a brief summary of any problems. Everything is good! The Radar Publishing service is down.</p> <p>More on these status can be found in the Status Monitoring section</p>"},{"location":"modules/walkthrough/#the-visualization-pages","title":"The Visualization Pages","text":"<p>These pages contain the user-facing functionality of the vision module.</p>"},{"location":"modules/walkthrough/#the-segmentation-page","title":"The Segmentation Page","text":"<p>The Segmentation page shows camera overlain with the current visual model output.  For Raivin modules, this will also includes the occupany grid at the bottom.  White points is unmatched, raw data from the radar; green points are raw data matched to segmentation masks.</p> <p>For the Maivin, its Segmentation Page does not include the Occupancy Grid at the bottom. </p>"},{"location":"modules/walkthrough/#the-occupancy-page-raivin-only","title":"The Occupancy Page (Raivin-only)","text":"<p>The Occupancy Page shows the raw, radar data, coloured by radar cross-section (RCS) size. </p>"},{"location":"modules/walkthrough/#the-gps-page","title":"The GPS Page","text":"<p>The GPS page shows an interactive map centered on the device's location.  This should be familiar to anyone who has used standard map web interfaces.  The map can be moved by dragging with left-mouse button (or touch with a touchscreen-enabled device).  The \"+\" and \"-\" buttons on the left will zoom-in and zoom-out on the map.  The \"Refresh\" button will re-center the map on the device's location.  The latitude and longitude are also reported on the web interface.</p>"},{"location":"modules/walkthrough/#the-imu-page","title":"The IMU Page","text":"<p>The IMU page shows the device's orientation in 3D.  My physically moving the device, it's virtual counterpart should move the same way.  Roll, pitch, and yaw values are reported.  If the device's virtual orientation does not match the physical orientation, keep the device's bottom flat and hit the \"Reset Orientation\" button.</p>"},{"location":"modules/walkthrough/#the-mcap-recording-page","title":"The MCAP Recording Page","text":"<p>The MCAP Recording Page manages the device's operational state as well as the current recordings on the device.  More information for this page can be found in the Recording section.</p>"}]}